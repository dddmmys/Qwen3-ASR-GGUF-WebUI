> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [arxiv.org](https://arxiv.org/html/2601.21337v1)

> In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognit......

###### Abstract

In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves state-of-the-art performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy–efficiency trade-off. Qwen3-ASR-0.6B can achieve an average time-to-first-token as low as 92ms and transcribe 2,000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.

1 Introduction
--------------

![](https://arxiv.org/html/2601.21337v1/figures/qwenasr-intro.png) Figure 1: Qwen3-ASR family includes all-in-one ASR models with advantages in multilingual, noisy speech recognition, singing voice recognition and inference efficiency, so as to a novel multilingual speech forced alignment model for predicting timestamps of words or sentences in ASR results.

In recent years, Automatic Speech Recognition (ASR) has transitioned from traditional end-to-end (E2E) paradigms, e.g., Transducer (Graves, [2012](https://arxiv.org/html/2601.21337v1#bib.bib8 "Sequence Transduction with Recurrent Neural Networks")) and AED (Chan et al., [2016](https://arxiv.org/html/2601.21337v1#bib.bib4 "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"); Radford et al., [2023](https://arxiv.org/html/2601.21337v1#bib.bib12 "Robust Speech Recognition via Large-Scale Weak Supervision")) to the Large Audio-Language Model (LALM) paradigm. Compared with traditional ASR, this paradigm can take advantage of the language modeling capabilities and world knowledge of large language models. The model first forms a high-level understanding of the audio signal and then generates transcription conditioned on this understanding, rather than relying solely on bottom-up acoustic pattern matching. Under this paradigm, issues that are relatively challenging for conventional ASR models—such as long-form transcription, robustness to noise, world-knowledge and named-entity recognition, as well as multilingual and dialectal coverage, can be addressed more naturally.

In real-world deployments, ASR systems are often required to output timestamps alongside transcripts (e.g., for subtitle generation). Prior work typically performs timestamping as a post-processing step using techniques such as CTC or CIF (Kürzinger et al., [2020](https://arxiv.org/html/2601.21337v1#bib.bib9 "CTC-Segmentation of Large Corpora for German End-to-End Speech Recognition"); Rastorgueva et al., [2023](https://arxiv.org/html/2601.21337v1#bib.bib13 "NeMo Forced Aligner and its application to word alignment for subtitle generation"); Shi et al., [2023](https://arxiv.org/html/2601.21337v1#bib.bib14 "Achieving timestamp prediction while recognizing with non-autoregressive end-to-end ASR model")). We would like to highlight that an LALM-based approach can yield more accurate and faster timestamp prediction at arbitrary temporal granularities, and that, by leveraging the multilingual capacity of LALMs, a single unified model can provide timestamp alignment across diverse languages.

In this report, we present the Qwen3-ASR family, including Qwen3-ASR-1.7B and Qwen3-ASR-0.6B - two all-in-one ASR models with language identification (LID) ability for 52 languages and dialects, and Qwen3-ForcedAligner-0.6B - the first lightweight LALM-based multilingual forced aligner supporting 11 languages and flexible timestamp prediction granularities. These model are posttrained from the strong foundation model of Qwen3-Omni (Xu et al., [2025b](https://arxiv.org/html/2601.21337v1#bib.bib28 "Qwen3-omni technical report")). For evaluating the performance of ASR models on benchmarks out of the open-sourced ones (ASR models at present have reached the limit of annotation errors on several test sets), we build a series of internal benchmarks covering more than complex acoustic environment, dialects, elders and kids speech and multilingual. Qwen3-ASR-1.7B achieves state-of-the-art (SOTA) performance among open-sourced ASR models and is competitive with the strongest proprietary commercial APIs. Qwen3-ASR-0.6B offers the best accuracy-model-size trade-off, making it a strong choice for on-device deployment. Qwen3-ForcedAligner-0.6B delivers highly accurate forced-alignment timestamps and inherits the key capabilities of Qwen3-ASR, including multilingual and long-form speech support, enabling scalable labeling of speech-transcript pairs.

The key features and contributions of the proposed Qwen3-ASR family models can be summarized as:

*   •
    
    Achieves state-of-the-art all-in-one ASR and LID performance. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B finely support 30 languages, 22 Chinese dialects ASR, and English from countries and regions worldwide. These two models also conduct robust speech recognition under complex environment, including but not limited to singing voice and song recognition, noise environment recognition and complex text patterns recognition.
    
*   •
    
    Presents novel speech force alignment architecture. To the best of our knowledge, we introduce the first Large Language Model based speech forced aligner that produces accurate timestamps at flexible granularities, including word, sentence, and paragraph levels. Compared with forced-alignment models at present including Montreal Forced Aligner (MFA) and NeMo Forced Aligner (NFA), Qwen3-ForcedAligner-0.6B fills the gap of an all-in-one, multilingual speech forced alignment solution, and completes an essential function of the Qwen3-ASR family.
    
*   •
    
    Open-sourced weights, powerful inference and finetune framework. Except weights of the 3 released models, we also open-source complete and easy-using code base for inference with multiple features as well as a finetuning recipe. We hope this unified toolkit facilitates the research and development of the ASR community.
    

2 Qwen3-ASR
-----------

### 2.1 Architecture

![](https://arxiv.org/html/2601.21337v1/x1.png) Figure 2: Architecture of AuT (left) and the overview of Qwen3-ASR (right).

Qwen3-ASR famliy models leverage Qwen3-Omni as foundation model, which proved to obtain strong audio understanding ability Xu et al. ([2025a](https://arxiv.org/html/2601.21337v1#bib.bib16 "Qwen3-Omni Technical Report")). Speech to recognize is first fed to AuT encoder, which is pretrained separately from Qwen3-Omni and Qwen3-ASR. As illustrated in Figure [2](https://arxiv.org/html/2601.21337v1#S2.F2 "Figure 2 ‣ 2.1 Architecture ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report")(left), AuT is an attention-encoder-decoder (AED) based ASR model which conducts 8 times downsampling to Fbank feature with 128 dimensions, yielding a 12.5Hz token rate audio encoder. We use dynamic flash attention window size ranging from 1s to 8s, which allows Qwen3-ASR to perform both streaming inference with short chunks and offline inference with long queries. The architecture of models we are releasing is illustrated as Figure [2](https://arxiv.org/html/2601.21337v1#S2.F2 "Figure 2 ‣ 2.1 Architecture ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report")(right) and detailed as below: Qwen3-ASR-1.7B is built with Qwen3-1.7B, a projector and the an AuT encoder with 300M parameters and 1024 hidden size. This model demonstrates strong performance in multilingual and dialect speech recognition, so as to robustness under complex acoustic environment and text patterns. Qwen3-ASR-0.6B is built with Qwen3-0.6B, a projector and an AuT encoder with 180M parameters and a hidden size of 896. We design this compact model to balance recognition accuracy and inference efficiency, while remaining highly competitive among sub-1B-parameter ASR models.

### 2.2 Training Strategies

The training process of Qwen3-ASR consists of AuT pretraining, Omni pretraining, and ASR post-training, where the first two stages are identical to those of Qwen3-Omni.

1.  (1)
    
    AuT pretraining. In this stage, we aim to obtain a pretrained encoder under the AED framework using large-scale labeled data. We leverage approximately 40 million hours of pseudo-labeled ASR data, where the majority is in Chinese and English. This pretrained encoder is shown to provide general and stable audio representations under dynamic attention window sizes.
    
2.  (2)
    
    Omni pretraining. We use the pretrained Qwen3-Omni model as the foundation model for ASR training. Omni pretraining is conducted on multi-task audio, vision, and text data. In this stage, both Qwen3-ASR-0.6B and Qwen3-ASR-1.7B are trained with 3 trillion tokens, acquiring multi-modal understanding capability. The detailed training pipeline follows (Xu et al., [2025a](https://arxiv.org/html/2601.21337v1#bib.bib16 "Qwen3-Omni Technical Report")).
    
3.  (3)
    
    ASR supervised finetuning (SFT). In the SFT stage, we perform style transfer on the ASR input/output format with a substantially smaller set of multilingual data that is disjoint from the pretraining corpus. Besides standard Chinese, English and multilingual ASR data, SFT stage also utilize non-speech data, streaming-enhancement data and context biasing data. Specifically, we train the model to be an ASR-only model that does not follow natural-language instructions in the prompt, in order to mitigate instruction injection and instruction-following failures. The output of Qwen3-ASR has outputs in two types for the given audio with and without recognizable human speech:
    
    Qwen3-ASR output style Output style 1: For recognizable speech  
    ¡—im_start—¿assistant language English¡asr_text¿Today we release models including Qwen3-ASR-1.7B.¡—im_end—¿ Output style 2: For no speech detected  
    ¡—im_start—¿assistant language None¡asr_text¿¡—im_end—¿
    
    Meanwhile, the model learns to utilize the context tokens inside the system prompt as background knowledge, allowing user to obtain customized ASR results.
    
4.  (4)
    
    ASR reinforcement learning (RL). At the last stage, we use Group Sequence Policy Optimization (GSPO, Zheng et al. ([2025](https://arxiv.org/html/2601.21337v1#bib.bib19 "Group Sequence Policy Optimization"))) for further improving the quality of recognition. It turns out that RL plays an essential role for models’ noise robustness, transcribing stability and ability to analyze difficult cases. The total data leveraged by RL stage is about 50k utterances including 35% Chinese and English data, 35% multilingual data and 30% functional data which aims at improving transcribing stability in complex environments.
    

### 2.3 Features

With architecture and training strategies introduced above, Qwen3-ASR family models are notable in aspects below:

Table 1: Features of the Qwen3-ASR model family. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support 52 languages and dialects, comprising 30 languages and 22 Chinese dialects. Qwen3-ForcedAligner-0.6B supports 11 languages. Seq. Len. denotes the maximum audio length for single inference in seconds, and NAR denotes non-autoregressive inference.

<table><tbody><tr><td>Model</td><td>Supported Languages</td><td>Supported Dialects</td><td>Inference Mode</td><td>Seq. Len.</td><td>Audio Types</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Qwen3-ASR-1.7B &amp; Qwen3-ASR-0.6B</td><td>Chinese (zh), English (en), Cantonese (yue), Arabic (ar), German (de), French (fr), Spanish (es), Portuguese (pt), Indonesian (id), Italian (it), Korean (ko), Russian (ru), Thai (th), Vietnamese (vi), Japanese (ja), Turkish (tr), Hindi (hi), Malay (ms), Dutch (nl), Swedish (sv), Danish (da), Finnish (fi), Polish (pl), Czech (cs), Filipino (fil), Persian (fa), Greek (el), Hungarian (hu), Macedonian (mk), Romanian (ro)</td><td>Anhui, Dongbei, Fujian, Gansu, Guizhou, Hebei, Henan, Hubei, Hunan, Jiangxi, Ningxia, Shandong, Shaanxi, Shanxi, Sichuan, Tianjin, Yunnan, Zhejiang. Cantonese (Hong Kong accent), Cantonese (Guangdong accent), Wu language, Minnan language.</td><td>Offline / Streaming</td><td>1200s</td><td>Speech, Singing Voice, Songs with BGM</td></tr><tr><td rowspan="3">Qwen3- ForcedAligner-0.6B</td><td>Chinese, English, Cantonese, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish</td><td rowspan="3">–</td><td rowspan="3">NAR</td><td rowspan="3">300s</td><td rowspan="3">Speech</td></tr></tbody></table>

1.  (1)
    
    Accurate Chinese and English ASR. Chinese and English account for the majority of the training data across all stages, and the model achieves leading Chinese and English recognition performance over multiple benchmarks compared with many competing systems.
    
2.  (2)
    
    Multilingual, multiple dialects supporting. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support 30 languages and 22 dialects, detailed in [Section 2.3](https://arxiv.org/html/2601.21337v1#S2.SS3 "2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
    
3.  (3)
    
    Long-form and streaming inference. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B naturally supports single speech no longer than 20 minutes and streaming/offline unified inference.
    
4.  (4)
    
    Singing voice and songs recognition. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B recognize singing voice and songs accurately. In addition to achieving strong singing-voice recognition, the Qwen3-ASR family also supports direct transcription of complete songs with background music (BGM), demonstrating robustness to accompaniment and complex musical mixtures.
    

### 2.4 Inference Efficiency

The speed benchmarks of Qwen3-ASR are conducted in two settings: offline batch inference and online asynchronous inference. The former is evaluated using vLLM’s offline batch generation, while the latter is evaluated with a multi-concurrency request setup based on vLLM Serve, which better reflects inference efficiency in industrial environments. All experiments are run with vLLM v0.14.0, with CUDA Graph enabled and bfloat16 precision for inference. The results in [Table 2](https://arxiv.org/html/2601.21337v1#S2.T2 "In 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report") show that, under different concurrency levels, Qwen3-ASR-0.6B can achieve an average Time-to-First-Token (TTFT) as low as 92ms. It reaches real-time factor (RTF) as low as 0.064 and throughput as high as 2000 at a concurrency of 128, which means it can process 2,000 seconds of audio per second.

Table 2: Efficiency of Qwen3-ASR family models. Qwen3-ASR-0.6B and Qwen3-ASR-1.7B support vLLM based inference in both offline batch and online asynchronous mode, while Qwen3-ForcedAligner-0.6B supports offline batch inference in PyTorch only. All measurements in the table are based on input audio of approximately 2 minutes for ASR and 1 minutes for FA in length, and all inference is performed on a single typical compute resource. Conc. denotes the concurrency level. TTFT p95 denotes the 95th percentile TTFT latency.

<table><tbody><tr><td rowspan="2">Model</td><td></td><td colspan="2">Offline</td><td colspan="4">Online async</td></tr><tr><td>Conc.</td><td>RTF</td><td>Throughput</td><td>TTFT avg. (ms)</td><td>TTFT p95 (ms)</td><td>RTF</td><td>Throughput</td></tr><tr><td rowspan="10">Qwen3-ASR-0.6B</td><td>1</td><td>0.00923</td><td>108.34</td><td>92</td><td>105</td><td>0.00940</td><td>106.38</td></tr><tr><td>2</td><td>0.01124</td><td>177.94</td><td>103</td><td>168</td><td>0.01108</td><td>180.51</td></tr><tr><td>4</td><td>0.01284</td><td>311.53</td><td>132</td><td>203</td><td>0.01224</td><td>326.80</td></tr><tr><td>8</td><td>0.01600</td><td>500.00</td><td>228</td><td>417</td><td>0.01472</td><td>543.48</td></tr><tr><td>16</td><td>0.02384</td><td>671.14</td><td>459</td><td>882</td><td>0.01936</td><td>826.45</td></tr><tr><td>32</td><td>0.03808</td><td>840.34</td><td>820</td><td>1575</td><td>0.02912</td><td>1098.90</td></tr><tr><td>64</td><td>0.06336</td><td>1010.10</td><td>1631</td><td>3196</td><td>0.04352</td><td>1470.59</td></tr><tr><td>128</td><td>0.11264</td><td>1136.36</td><td>3210</td><td>6195</td><td>0.06400</td><td>2000.00</td></tr><tr><td>256</td><td>0.21504</td><td>1190.48</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>512</td><td>0.44544</td><td>1149.43</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td rowspan="10">Qwen3-ASR-1.7B</td><td>1</td><td>0.01482</td><td>67.48</td><td>102</td><td>113</td><td>0.01483</td><td>67.43</td></tr><tr><td>2</td><td>0.01540</td><td>129.87</td><td>117</td><td>170</td><td>0.01530</td><td>130.72</td></tr><tr><td>4</td><td>0.01712</td><td>233.64</td><td>135</td><td>192</td><td>0.01688</td><td>236.97</td></tr><tr><td>8</td><td>0.02072</td><td>386.10</td><td>224</td><td>382</td><td>0.02000</td><td>400.00</td></tr><tr><td>16</td><td>0.02896</td><td>552.49</td><td>443</td><td>791</td><td>0.02640</td><td>606.06</td></tr><tr><td>32</td><td>0.04608</td><td>694.44</td><td>847</td><td>1570</td><td>0.03968</td><td>806.45</td></tr><tr><td>64</td><td>0.07360</td><td>869.57</td><td>1597</td><td>2942</td><td>0.06208</td><td>1030.93</td></tr><tr><td>128</td><td>0.13056</td><td>980.39</td><td>3392</td><td>6227</td><td>0.10496</td><td>1219.51</td></tr><tr><td>256</td><td>0.24320</td><td>1052.63</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>512</td><td>0.50176</td><td>1020.41</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td rowspan="8">Qwen3-ForcedAligner-0.6B</td><td>1</td><td>0.00889</td><td>112.49</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2</td><td>0.00232</td><td>862.07</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>4</td><td>0.00432</td><td>925.93</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>8</td><td>0.00832</td><td>961.54</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>16</td><td>0.01696</td><td>943.40</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>32</td><td>0.03584</td><td>892.86</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>64</td><td>0.08192</td><td>781.25</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>128</td><td>0.19712</td><td>649.35</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>

3 Qwen3-ForcedAligner
---------------------

### 3.1 Overview

Qwen3-ForcedAligner-0.6B aims to estimate the start and end timestamps of each word or character in a speech, given the corresponding transcript. Qwen3-ForcedAligner-0.6B reframes the forced alignment task within a slot-filling formulation. Specifically, given a speech and a transcript augmented with special tokens [time] that denote word-level or character-level start and end timestamp slots, Qwen3-ForcedAligner-0.6B directly predicts the corresponding discrete timestamp indices for each slot.

The key features and contributions of Qwen3-ForcedAligner-0.6B can be summarized as:

*   •
    
    Accurate Timestamp Prediction. Qwen3-ForcedAligner-0.6B exhibits substantially lower timestamp prediction shifts, achieving a relative reduction of 67%~77% in accumulated average shift on the human-labeled test datasets compared with other forced alignment methods.
    
*   •
    
    Broad Application Scenarios. Qwen3-ForcedAligner-0.6B supports speech in 11 languages with durations of up to 300 seconds, including the cross-lingual scenarios, and allows users to flexibly customize timestamp prediction for any word or character.
    
*   •
    
    Fast Inference Speed. Qwen3-ForcedAligner-0.6B abandons the next-token prediction paradigm and adopts non-autoregressive (NAR) inference for timestamp prediction.
    

### 3.2 Model Design

![](https://arxiv.org/html/2601.21337v1/figures/qwen3asr_fa.png) Figure 3: Illustration of Qwen3-ForcedAligner-0.6B. During training, randomly masked timestamp slots with are dynamically inserted into the token sequence to represent word or character boundaries. The combined sequence is fed into Qwen3-0.6B LLM, and a timestamp prediction layer predicts the corresponding timestamp indices for each slot. Supervision is applied with cross‑entropy loss on synchronously aligned label and output sequences.

As shown in Figure [3](https://arxiv.org/html/2601.21337v1#S3.F3 "Figure 3 ‣ 3.2 Model Design ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"), Qwen3-ForcedAligner-0.6B employs a pretrained AuT encoder to process the input speech signal and obtain speech embeddings. The transcript is reformatted by appending start and end timestamp labels to each word or character, after which each timestamp label is replaced with a special token [time] and fed into the tokenizer. Moreover, the timestamp labels in the transcript are discretized into indices by dividing each timestamp value by the 80ms frame duration of the AuT encoder output. Speech and text embedding sequences are processed by the Qwen3-0.6B LLM, followed by a timestamp prediction linear layer that predicts timestamp indices for the entire input sequence. In this work, the maximum number of classes is 3,750, corresponding to support for speech inputs of up to 300s.

The AuT encoder and the multilingual Qwen3-0.6B LLM jointly provide Qwen3-ForcedAligner-0.6B with multilingual and cross-lingual capabilities. Specifically, the AuT encoder, pretrained on a large-scale multilingual corpus, generates effective frame-level speech embeddings for multiple languages, while the multilingual Qwen3-0.6B LLM handles semantic information across different languages. In addition, the special token [time] and the timestamp prediction layer do not rely on language-specific phoneme sets or dictionaries. Details can be found in Mu et al. ([2026b](https://arxiv.org/html/2601.21337v1#bib.bib20 "LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech")).

### 3.3 Training Strategies

Training Qwen3-ForcedAligner-0.6B requires word-level or character-level timestamp labels for a large number of speech–transcript pairs. However, because manual annotation is prohibitively expensive, we use pseudo-timestamp labels generated by the Montreal forced aligner (MFA) McAuliffe et al. ([2017](https://arxiv.org/html/2601.21337v1#bib.bib10 "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi")), which is among the most accurate existing forced alignment methods. It is important to note that MFA pseudo-labels inherently contain noise and systematic shifts. Qwen3‑ForcedAligner does not simply replicate MFA outputs; instead, it distills and smooths these pseudo-labels, resulting in more stable timestamp predictions with reduced shift.

LALMs typically use a training scheme in which the last token of the output sequence and the first token of the label sequence are removed, creating a one-position offset between the two sequences; the cross-entropy loss is then computed to implement the standard next-token prediction paradigm. However, this paradigm is not suitable for filling timestamp slots. Qwen3-ForcedAligner-0.6B employs causal training, keeping the output and label sequences non-shifted, which allows the model to explicitly recognize timestamp slots during training and predict the timestamp indices to fill them. Moreover, causal training enables Qwen3-ForcedAligner-0.6B to incorporate prior contextual information when predicting the timestamp for the current slot, ensuring global consistency in timestamp prediction. The cross-entropy loss is computed only in the timestamp slots, thereby focusing the training objective of Qwen3-ForcedAligner-0.6B on timestamp slot filling.

In addition, Qwen3-ForcedAligner-0.6B employs a dynamic slot insertion strategy during training to enhance its generalization capability. Specifically, for each word or character in a sample, the model randomly determines whether to insert start and end timestamp slots afterward.

### 3.4 Inference and Usability

Since the token sequences remain non-shifted during training, users can insert start and end timestamp slots after any word or character, and Qwen3-ForcedAligner-0.6B uses non-autoregressive (NAR) decoding to predict the timestamp indices for all slots in the transcript simultaneously. Once the timestamp indices are obtained, multiplying each index by 80ms recovers the actual predicted timestamps.

The speed benchmark for Qwen3-ForcedAligner is conducted with FlashAttention and bfloat16. Since the model is non-autoregressive, the inference speed difference between Transformers and vLLM is relatively small; therefore, all our benchmarks are run with Transformers. The results in [Table 2](https://arxiv.org/html/2601.21337v1#S2.T2 "In 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report") show that the model can maintain an RTF close to 0.001 even under high concurrency, i.e., it can process 1,000 seconds of audio per second.

4 Experiments
-------------

### 4.1 Evaluation Details

Baseline Systems. To validate the Qwen3-ASR family, we conduct comparative evaluations against state-of-the-art (SOTA) closed-source ASR APIs and widely used open-source models. Specifically, we compare Qwen3-ASR with three leading proprietary services: GPT-4o-Transcribe (OpenAI, [2024](https://arxiv.org/html/2601.21337v1#bib.bib11 "Hello GPT-4o")), Gemini-2.5-Pro (Comanici et al., [2025](https://arxiv.org/html/2601.21337v1#bib.bib7 "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities")), and Doubao-ASR (Bai et al., [2024](https://arxiv.org/html/2601.21337v1#bib.bib2 "Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition")). We further include several multilingual open-source baselines, namely Whisper-large-v3 (Radford et al., [2023](https://arxiv.org/html/2601.21337v1#bib.bib12 "Robust Speech Recognition via Large-Scale Weak Supervision")), FunASR-MLT-Nano (An et al., [2025](https://arxiv.org/html/2601.21337v1#bib.bib1 "Fun-ASR Technical Report")), and GLM-ASR-Nano (Z.ai, [2025](https://arxiv.org/html/2601.21337v1#bib.bib18 "GLM asr 2512")). Together, these baselines represent strong commercial systems and competitive open-source alternatives, enabling a comprehensive evaluation of Qwen3-ASR under representative real-world conditions.

Benchmark Introduction. We adopt a four-part evaluation protocol to measure the speech recognition performance of the proposed Qwen3-ASR series:

1.  1.
    
    Public benchmarks (English and Chinese). We evaluate a broad set of public benchmarks (Conneau et al., [2023](https://arxiv.org/html/2601.21337v1#bib.bib24 "Fleurs: few-shot learning evaluation of universal representations of speech"); Ardila et al., [2020](https://arxiv.org/html/2601.21337v1#bib.bib25 "Common voice: a massively-multilingual speech corpus"); Zhang et al., [2022](https://arxiv.org/html/2601.21337v1#bib.bib26 "Wenetspeech: a 10000+ hours multi-domain mandarin corpus for speech recognition"); Panayotov et al., [2015](https://arxiv.org/html/2601.21337v1#bib.bib27 "Librispeech: an asr corpus based on public domain audio books"); Dai et al., [2025](https://arxiv.org/html/2601.21337v1#bib.bib22 "Wenetspeech-chuan: a large-scale sichuanese corpus with rich annotation for dialectal speech processing"); Li et al., [2025](https://arxiv.org/html/2601.21337v1#bib.bib23 "Wenetspeech-yue: a large-scale cantonese speech corpus with multi-dimensional annotation")) and report the results separately for subsets of English, standard Mandarin and Chinese dialects, including two recently released benchmarks.
    
2.  2.
    
    Internal robustness suite. We stress-test the model under challenging real-world conditions using a comprehensive in-house suite, covering English speech from multiple countries and accents (16 accent groups in total), 22 Chinese dialect varieties, and difficult scenarios including elderly and children’s speech, extremely low signal-to-noise (SNR) ratios, nonfluent and tongue-twister-like repetitive speech, and multi-speaker Chinese conversational speech. These settings enable a systematic assessment of robustness to accent/dialect variability and complex acoustic and linguistic conditions.
    
3.  3.
    
    Multilingual evaluation. The model supports ASR for 30 languages. We evaluate on Common Voice, Fleurs, MLS, MLC-SLM (Mu et al., [2026a](https://arxiv.org/html/2601.21337v1#bib.bib21 "Summary on The Multilingual Conversational Speech Language Model Challenge: Datasets, Tasks, Baselines, and Methods")), and an internally curated test set spanning 15 languages. The language inventory of each benchmark is specified in [Section 2.3](https://arxiv.org/html/2601.21337v1#S2.SS3 "2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"). Since Fleurs covers a particularly large and diverse set of languages, we additionally report results on progressively expanded language subsets grouped by language popularity and practical usage for a more fine-grained characterization. Meaning while, we evaluate the language identification performance on the multilingual open-source benchmarks.
    
4.  4.
    
    Singing voice recognition. We evaluate singing voice transcription on both public benchmarks and an internal test set. In the internal evaluation, we emphasize long-form transcription where an entire song is provided as a single input, to assess robustness to long-duration audio as well as the distinctive acoustic and rhythmic properties of singing.
    

Evaluation Metrics. For recognition accuracy, we report either word error rate (WER) or character error rate (CER) depending on the language. We use CER for character-based languages (e.g., Mandarin Chinese, Cantonese, and Korean) and WER for word-delimited languages (e.g., English, German, and French). When aggregated results are needed (e.g., average performance across multiple languages or dialects), we report the macro-average (i.e., the unweighted mean across languages/dialects). The best result in each table is highlighted in bold. In addition, when Qwen3-ASR-0.6B is the best-performing model after excluding the larger Qwen3-ASR-1.7B, we also highlight it in bold.

For language identification, we report language identification accuracy.

For timestamp accuracy, Qwen3-ForcedAligner uses Accumulated Average Shift (AAS Shi et al. ([2023](https://arxiv.org/html/2601.21337v1#bib.bib14 "Achieving timestamp prediction while recognizing with non-autoregressive end-to-end ASR model"))), where lower values indicate more accurate timestamp predictions. AAS is defined as the mean absolute difference between predicted timestamps and reference timestamps over all timestamp slots in the evaluated datasets:

<table><tbody><tr><td></td><td><math alttext="\mathrm{AAS}=\frac{1}{N}\sum_{i=1}^{N}\left|\hat{n}_{i}-n_{i}\right|," display="block" intent=":literal"><semantics><mrow><mrow><mi>AAS</mi><mo>=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mo>|</mo><mrow><msub><mover accent="true"><mi>n</mi><mo>^</mo></mover><mi>i</mi></msub><mo>−</mo><msub><mi>n</mi><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{AAS}=\frac{1}{N}\sum_{i=1}^{N}\left|\hat{n}_{i}-n_{i}\right|,</annotation></semantics></math></td><td></td><td rowspan="1">(1)</td></tr></tbody></table>

where  is the total number of timestamp slots,  denotes the timestamp predicted by Qwen3-ForcedAligner for slot , and  is the corresponding reference timestamp obtained from Montreal Forced Aligner (MFA) or manual annotations.

### 4.2 English & Chinese ASR Performance

#### 4.2.1 Opensource ASR Benchmarks

As shown in [Table A.1](https://arxiv.org/html/2601.21337v1#A1.T1 "In Appendix ‣ 6 Authors ‣ 5 Conclusion ‣ 4.6 Precision of Timestamps ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"), Qwen3-ASR delivers consistently strong performance across English, Mandarin Chinese, and multiple Chinese dialect benchmarks. It is competitive with leading commercial APIs while substantially outperforming widely used open-source baselines. Scaling from Qwen3-ASR-0.6B to Qwen3-ASR-1.7B yields clear and stable gains, indicating that the model benefits effectively from increased capacity.

On English benchmarks, Qwen3-ASR performs particularly well on diverse, real-world data (e.g., crowd-sourced or web-collected speech), where distribution shift is more pronounced than in read-speech settings. In these cases, Qwen3-ASR-1.7B achieves the strongest overall results on several datasets, while remaining close to the best-performing systems on standard academic evaluations such as LibriSpeech. Compared with commercial APIs, whose performance can vary substantially across datasets, Qwen3-ASR shows more consistent accuracy across a broad range of English conditions.

On Mandarin Chinese, Qwen3-ASR demonstrates a clear advantage. It delivers the best overall performance on most Mandarin benchmarks in the table and remains reliable on more challenging large-scale evaluations. Notably, on WenetSpeech, which contains diverse acoustic environments and meeting-style speech, Qwen3-ASR outperforms the available baselines by a large margin.

On Chinese dialect benchmarks, Qwen3-ASR maintains strong accuracy under substantial pronunciation and lexical variation. It consistently ranks among the top systems across Cantonese and other dialect datasets, and performs particularly well on more challenging long-utterance settings, demonstrating robustness beyond short, clean test conditions. While a small number of dialect-specific cases favor specialized commercial APIs, Qwen3-ASR remains highly competitive overall and provides a strong general-purpose solution across dialects without per-dialect customization.

Overall, [Table A.1](https://arxiv.org/html/2601.21337v1#A1.T1 "In Appendix ‣ 6 Authors ‣ 5 Conclusion ‣ 4.6 Precision of Timestamps ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report") highlights three key advantages of Qwen3-ASR: (i) strong cross-domain generalization on English benchmarks beyond curated read speech, (ii) state-of-the-art accuracy on Mandarin Chinese across multiple public datasets including large-scale, noisy meeting-style speech, and (iii) robust handling of Chinese dialects, with especially strong performance on Cantonese and long/short dialectal speech. These findings demonstrate that Qwen3-ASR delivers strong, reproducible performance across diverse public benchmarks, while also remaining competitive with top-tier closed-source APIs.

Table 3: Evaluation on English, Mandarin Chinese, and a range of Chinese dialect benchmarks. For the commercial APIs and the open-source Whisper-large-v3 model, we obtained results by running inference on the test sets ourselves due to the absence of published numbers; for FunASR-MLT-Nano, we report the results from its official technical report. ”N/A” denotes we cannot get a reasonable result by the official API. ”–” indicates that the corresponding benchmark result is not reported.

<table><tbody><tr><td colspan="2"></td><td></td><td></td><td>Doubao-ASR</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9">English (en)</td></tr><tr><td></td><td>LibriSpeech</td><td rowspan="2">1.39—3.75</td><td rowspan="2">2.89—3.56</td><td rowspan="2">2.78—5.70</td><td rowspan="2">1.51—3.97</td><td rowspan="2">1.68—4.03</td><td rowspan="2">2.11—4.55</td><td rowspan="2">1.63—3.38</td></tr><tr><td></td><td>clean — other</td></tr><tr><td></td><td>GigaSpeech</td><td>25.50</td><td>9.37</td><td>9.55</td><td>9.76</td><td>-</td><td>8.88</td><td>8.45</td></tr><tr><td></td><td>CV-en</td><td>9.08</td><td>14.49</td><td>13.78</td><td>9.90</td><td>9.90</td><td>9.92</td><td>7.39</td></tr><tr><td></td><td>Fleurs-en</td><td>2.40</td><td>2.94</td><td>6.31</td><td>4.08</td><td>5.49</td><td>4.39</td><td>3.35</td></tr><tr><td></td><td>MLS-en</td><td>5.12</td><td>3.68</td><td>7.09</td><td>4.87</td><td>-</td><td>6.00</td><td>4.58</td></tr><tr><td></td><td>Tedlium</td><td>7.69</td><td>6.15</td><td>4.91</td><td>6.84</td><td>-</td><td>3.85</td><td>4.50</td></tr><tr><td></td><td>VoxPopuli</td><td>10.29</td><td>11.36</td><td>12.12</td><td>12.05</td><td>-</td><td>9.96</td><td>9.15</td></tr><tr><td colspan="9">Chinese (zh)</td></tr><tr><td></td><td>WenetSpeech</td><td rowspan="2">15.30—32.27</td><td rowspan="2">14.43—13.47</td><td rowspan="2">N/A</td><td rowspan="2">9.86—19.11</td><td rowspan="2">6.35—-</td><td rowspan="2">5.97—6.88</td><td rowspan="2">4.97—5.88</td></tr><tr><td></td><td>net — meeting</td></tr><tr><td></td><td>AISHELL-2-test</td><td>4.24</td><td>11.62</td><td>2.85</td><td>5.06</td><td>-</td><td>3.15</td><td>2.71</td></tr><tr><td></td><td>SpeechIO</td><td>12.86</td><td>5.30</td><td>2.93</td><td>7.56</td><td>-</td><td>3.44</td><td>2.88</td></tr><tr><td></td><td>Fleurs-zh</td><td>2.44</td><td>2.71</td><td>2.69</td><td>4.09</td><td>3.51</td><td>2.88</td><td>2.41</td></tr><tr><td></td><td>CV-zh</td><td>6.32</td><td>7.70</td><td>5.95</td><td>12.91</td><td>6.20</td><td>6.89</td><td>5.35</td></tr><tr><td colspan="9">Chinese Dialect</td></tr><tr><td></td><td>KeSpeech</td><td>26.87</td><td>24.71</td><td>5.27</td><td>28.79</td><td>-</td><td>7.08</td><td>5.10</td></tr><tr><td></td><td>Fleurs-yue</td><td>4.98</td><td>9.43</td><td>4.98</td><td>9.18</td><td>-</td><td>5.79</td><td>3.98</td></tr><tr><td></td><td>CV-yue</td><td>11.36</td><td>18.76</td><td>13.20</td><td>16.23</td><td>-</td><td>9.50</td><td>7.57</td></tr><tr><td></td><td>CV-zh-tw</td><td>6.32</td><td>7.31</td><td>4.06</td><td>7.84</td><td>-</td><td>5.59</td><td>3.77</td></tr><tr><td></td><td>WenetSpeech-Yue</td><td rowspan="2">15.62—25.29</td><td rowspan="2">25.19—11.23</td><td rowspan="2">9.74—11.40</td><td rowspan="2">32.26—46.64</td><td rowspan="2">-—-</td><td rowspan="2">7.54—9.92</td><td rowspan="2">5.82—8.85</td></tr><tr><td></td><td>short — long</td></tr><tr><td></td><td>WenetSpeech-Chuan</td><td rowspan="2">34.81—53.98</td><td rowspan="2">43.79—67.30</td><td rowspan="2">11.40—20.20</td><td rowspan="2">14.35—26.80</td><td rowspan="2">-—-</td><td rowspan="2">13.92—24.45</td><td rowspan="2">11.99—21.63</td></tr><tr><td></td><td>easy — hard</td></tr></tbody></table>

#### 4.2.2 Internal ASR Benchmarks

Table 4: Evaluation on internal English and Chinese test sets covering multiple accents and dialects, as well as challenging acoustic conditions and difficult speaking scenarios.

<table><tbody><tr><td colspan="2"></td><td></td><td></td><td>Doubao-ASR</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9">Accented English</td></tr><tr><td></td><td>Dialog-Accented English</td><td>28.56</td><td>23.85</td><td>20.41</td><td>21.30</td><td>19.96</td><td>16.62</td><td>16.07</td></tr><tr><td colspan="9">Chinese Mandarin</td></tr><tr><td></td><td>Elders&amp;Kids</td><td>14.27</td><td>36.93</td><td>4.17</td><td>10.61</td><td>4.54</td><td>4.48</td><td>3.81</td></tr><tr><td></td><td>ExtremeNoise</td><td>36.11</td><td>29.06</td><td>17.04</td><td>63.17</td><td>36.55</td><td>17.88</td><td>16.17</td></tr><tr><td></td><td>TongueTwister</td><td>20.87</td><td>4.97</td><td>3.47</td><td>16.63</td><td>9.02</td><td>4.06</td><td>2.44</td></tr><tr><td></td><td>Dialog-Mandarin</td><td>20.73</td><td>12.50</td><td>6.61</td><td>14.01</td><td>7.32</td><td>7.06</td><td>6.54</td></tr><tr><td colspan="9">Chinese Dialect</td></tr><tr><td></td><td>Dialog-Cantonese</td><td>16.05</td><td>14.98</td><td>7.56</td><td>31.04</td><td>5.85</td><td>4.80</td><td>4.12</td></tr><tr><td></td><td>Dialog-Chinese Dialects</td><td>45.37</td><td>47.70</td><td>19.85</td><td>44.55</td><td>19.41</td><td>18.24</td><td>15.94</td></tr></tbody></table>

*   •
    
    Dialect coverage: Results for Dialog-Accented English are averaged over 16 accents, and results for Dialog-Chinese Dialects are averaged over 22 Chinese dialects. Detailed category definitions are provided in [Section 2.3](https://arxiv.org/html/2601.21337v1#S2.SS3 "2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
    

To further assess robustness in realistic deployment settings, we evaluate Qwen3-ASR on our internal robustness suite; results are summarized in Table [4](https://arxiv.org/html/2601.21337v1#S4.T4 "Table 4 ‣ 4.2.2 Internal ASR Benchmarks ‣ 4.2 English & Chinese ASR Performance ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"). Qwen3-ASR delivers consistently strong performance across all subsets, and scaling from 0.6B to 1.7B yields stable gains. In the accented-English evaluation, Qwen3-ASR achieves the lowest WER among all compared systems, surpassing both commercial APIs and open-source baselines, indicating better generalization to accent variation. On Mandarin, Qwen3-ASR-1.7B performs best across all evaluated subsets, demonstrating robustness under difficult acoustic and speaking conditions. In dialectal Chinese, Qwen3-ASR again achieves the best results on both conversational Cantonese and the aggregated 22-dialect evaluation; the gains are particularly pronounced in the multi-dialect mixture, highlighting improved robustness as linguistic diversity increases. Overall, these internal results are consistent with the public-benchmark findings and further confirm that Qwen3-ASR provides reliable recognition quality in high-variability scenarios.

### 4.3 Multilingual ASR and Language Identification

#### 4.3.1 Multilingual ASR Performance

Table 5: Evaluation of multilingual ASR systems on a comprehensive set of benchmark datasets.

<table><tbody><tr><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="7">Open-sourced Benchmarks</td></tr><tr><td></td><td>MLS</td><td>13.32</td><td>8.62</td><td>28.70</td><td>13.19</td><td>8.55</td></tr><tr><td></td><td>CommonVoice</td><td>19.40</td><td>10.77</td><td>17.25</td><td>12.75</td><td>9.18</td></tr><tr><td></td><td>MLC-SLM</td><td>34.93</td><td>15.68</td><td>29.94</td><td>15.84</td><td>12.74</td></tr><tr><td></td><td>Fleurs</td><td>16.08</td><td>5.27</td><td>10.03</td><td>7.57</td><td>4.90</td></tr><tr><td></td><td>Fleurs<sup>†</sup></td><td>20.05</td><td>6.85</td><td>31.89</td><td>10.37</td><td>6.62</td></tr><tr><td></td><td>Fleurs<sup>†</sup><sup>†</sup></td><td>24.83</td><td>8.16</td><td>47.84</td><td>21.80</td><td>12.60</td></tr><tr><td colspan="7">Qwen-ASR Internal Benchmarks</td></tr><tr><td></td><td>News-Multilingual</td><td>49.40</td><td>14.80</td><td>65.07</td><td>17.39</td><td>12.80</td></tr></tbody></table>

*   •
    
    Language coverage: MLS includes 8 languages: {da, de, en, es, fr, it, pl, pt}.  
    CommonVoice includes 13 languages: {en, zh, yue, zh_TW, ar, de, es, fr, it, ja, ko, pt, ru}.  
    MLC-SLM includes 11 languages: {en, fr, de, it, pt, es, jp, ko, ru, th, vi}.  
    Fleurs includes 12 languages: {en, zh, yue, ar, de, es, fr, it, ja, ko, pt, ru}.  
    Fleurs† includes 8 additional languages beyond Fleurs: {hi, id, ms, nl, pl, th, tr, vi}.  
    Fleurs†† includes 10 additional languages beyond Fleurs†: {cs, da, el, fa, fi, fil, hu, mk, ro, sv}.  
    News-Multilingual includes 15 languages: {ar, de, es, fr, hi, id, it, ja, ko, nl, pl, pt, ru, th, vi}.
    

In this part, we illustrate the multilingual ASR performance of the Qwen3-ASR series on a broad set of public benchmarks as well as our internal multilingual news evaluation (Table [5](https://arxiv.org/html/2601.21337v1#S4.T5 "Table 5 ‣ 4.3.1 Multilingual ASR Performance ‣ 4.3 Multilingual ASR and Language Identification ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report")). Overall, Qwen3-ASR-1.7B achieves the best average performance on most test settings, showing strong generalization across languages and domains, while Qwen3-ASR-0.6B provides a competitive lightweight alternative.

On MLS, Common Voice and MLC-SLM, Qwen3-ASR-1.7B consistently outperforms the evaluated open-source baselines, including the widely used Whisper-large-v3, and substantially surpasses smaller multilingual models. On Fleurs, which spans more languages and diverse recording conditions, Qwen3-ASR-1.7B achieves the best performance on the 12- and 20-language subsets. However, relative to Whisper-large-v3, its performance degrades on the full 30-language setting, indicating room for improvement in handling increased linguistic diversity and long-tail languages. Nevertheless, Qwen3-ASR-1.7B remains markedly better than the 0.6B variant, suggesting that model scaling improves robustness in more challenging multilingual regimes.

Finally, on our internal News-Multilingual benchmark, Qwen3-ASR-1.7B achieves the best overall performance, demonstrating stronger robustness to domain shift (e.g., broadcast/news-style speech) than all baselines. Overall, these results indicate effective scaling behavior and strong multilingual recognition across both public and internal evaluations. Per-language results for the Qwen3-ASR family are provided in the Appendix.

#### 4.3.2 Language Identification Performance

Table 6: Language identification accuracy (%)  on open-source multilingual test sets.

<table><tbody><tr><td></td><td>Whisper-large-v3</td><td>Qwen3-ASR-0.6B</td><td>Qwen3-ASR-1.7B</td></tr><tr><td>MLS</td><td>99.9</td><td>99.3</td><td>99.9</td></tr><tr><td>CommonVoice</td><td>92.7</td><td>98.2</td><td>98.7</td></tr><tr><td>MLC-SLM</td><td>89.2</td><td>92.7</td><td>94.1</td></tr><tr><td>Fleurs</td><td>94.6</td><td>97.1</td><td>98.7</td></tr><tr><td>Avg.</td><td>94.1</td><td>96.8</td><td>97.9</td></tr></tbody></table>

*   •
    
    Language coverage: The language sets follow [Table 5](https://arxiv.org/html/2601.21337v1#S4.T5 "In 4.3.1 Multilingual ASR Performance ‣ 4.3 Multilingual ASR and Language Identification ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"). Here, Fleurs corresponds to Fleurs†† in [Table 5](https://arxiv.org/html/2601.21337v1#S4.T5 "In 4.3.1 Multilingual ASR Performance ‣ 4.3 Multilingual ASR and Language Identification ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report") and covers 30 languages.
    

Following the output template in [Section 2.2](https://arxiv.org/html/2601.21337v1#S2.SS2 "2.2 Training Strategies ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"), Qwen3-ASR not only decodes speech into text, but also performs language identification (LID) via natural-language prompting before ASR decoding. In this section, we evaluate LID accuracy on 4 multilingual benchmarks: Fleurs (30 languages), MLS (9 languages), CommonVoice (13 languages), MLC-SLM (11 languages); the covered languages are detailed in [Section 2.3](https://arxiv.org/html/2601.21337v1#S2.SS3 "2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"). As shown in Table [6](https://arxiv.org/html/2601.21337v1#S4.T6 "Table 6 ‣ 4.3.2 Language Identification Performance ‣ 4.3 Multilingual ASR and Language Identification ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"), we compare Qwen3-ASR-0.6B and Qwen3-ASR-1.7B with Whisper-large-v3, a strong multilingual ASR model with built-in LID capability. Both Qwen3-ASR models outperform Whisper-large-v3, demonstrating stable and effective language identification across these mainstream languages. Most remaining errors on Fleurs stem from confusions between Malay (ms) and Indonesian (id), two closely related languages with high acoustic similarity.

### 4.4 Singing Voice & Songs Recognition Performance

Table 7: Singing-voice and song-transcription results. WER (%) is reported for singing-only benchmarks and long-form songs with background music. ”N/A” indicates that the model does not support long-form song recognition due to the poor performance.

<table><tbody><tr><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="7">Singing</td><td></td><td></td></tr><tr><td></td><td>M4Singer</td><td>16.77</td><td>20.88</td><td>7.88</td><td>13.58</td><td>7.29</td><td>5.98</td><td></td></tr><tr><td></td><td>MIR-1k-vocal</td><td>11.87</td><td>9.85</td><td>6.56</td><td>11.71</td><td>8.17</td><td>6.25</td><td></td></tr><tr><td></td><td>Opencpop</td><td>7.93</td><td>6.49</td><td>3.80</td><td>9.52</td><td>2.98</td><td>3.08</td><td></td></tr><tr><td></td><td>Popcs</td><td>32.84</td><td>15.13</td><td>8.97</td><td>13.77</td><td>9.42</td><td>8.52</td><td></td></tr><tr><td colspan="7">Songs with BGM</td><td></td><td></td></tr><tr><td></td><td>EntireSongs-en</td><td>30.71</td><td>12.18</td><td>33.51</td><td>N/A</td><td>N/A</td><td>14.60</td><td></td></tr><tr><td></td><td>EntireSongs-zh</td><td>34.86</td><td>18.68</td><td>23.99</td><td>N/A</td><td>N/A</td><td>13.91</td><td></td></tr></tbody></table>

Table [7](https://arxiv.org/html/2601.21337v1#S4.T7 "Table 7 ‣ 4.4 Singing Voice & Songs Recognition Performance ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report") reports results for singing-voice transcription and long-form song transcription with background music. Overall, Qwen3-ASR-1.7B is robust to melody-induced pronunciation variation and musical accompaniment, outperforming most commercial APIs and open-source baselines across the evaluated sets. For singing-only benchmarks, it achieves the best performance for M4Singer, MIR-1k-vocal, and Popcs, while remaining competitive for Opencpop (second to FunASR-MLT-Nano by a small margin), indicating strong generalization across singing styles and recording conditions with reduced sensitivity to pitch drift, phoneme elongation, and rhythmic lyric variation. For full songs with background music, Qwen3-ASR-1.7B substantially outperforms open-source baselines; Whisper-large-v3 and FunASR-MLT-Nano degrade markedly in long-form, music-mixed settings. It achieves high accuracy for both English and Chinese songs, ranking first on the Chinese set and remaining competitive with the best commercial system on the English set, suggesting that Qwen3-ASR is well suited to realistic music-containing scenarios and background-music-robust and narrows the gap between speech ASR and singing/song transcription.

### 4.5 Streaming Speech Recognition

This section evaluates Qwen3-ASR-1.7B and Qwen3-ASR-0.6B in both offline and streaming inference modes. Benefiting from the dynamic attention-window mechanism, the Qwen3-ASR family supports streaming inference naturally. Table [8](https://arxiv.org/html/2601.21337v1#S4.T8 "Table 8 ‣ 4.5 Streaming Speech Recognition ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report") reports results on three open-source test sets using a 2-second chunk size, a 5-token fallback, and keeping the last four chunks unfixed. Overall, Qwen3-ASR provides a unified model for offline and streaming use, while streaming inference preserves strong recognition accuracy.

Table 8: ASR performance of the two inference modes on three open-source benchmarks.

<table><tbody><tr><td>Model</td><td>Infer. Mode</td><td>Librispeech</td><td>Fleurs-en</td><td>Fleurs-zh</td><td>Avg.</td></tr><tr><td rowspan="2">Qwen3-ASR-1.7B</td><td>Offline</td><td>1.63 — 3.38</td><td>3.35</td><td>2.41</td><td>2.69</td></tr><tr><td>Streaming</td><td>1.95 — 4.51</td><td>4.02</td><td>2.84</td><td>3.33</td></tr><tr><td rowspan="2">Qwen3-ASR-0.6B</td><td>Offline</td><td>2.11 — 4.55</td><td>4.39</td><td>2.88</td><td>3.48</td></tr><tr><td>Streaming</td><td>2.54 — 6.27</td><td>5.38</td><td>3.40</td><td>4.40</td></tr></tbody></table>

### 4.6 Precision of Timestamps

Table 9: Accumulated Average Shift (AAS, ms)  of Qwen3-ForcedAligner-0.6B and competing forced-alignment methods on MFA-labeled and human-labeled test sets.

<table><tbody><tr><td></td><td>Monotonic-Aligner</td><td>NFA</td><td>WhisperX</td><td>Qwen3-ForcedAligner-0.6B</td></tr><tr><td colspan="5">MFA-Labeled Raw</td></tr><tr><td>Chinese</td><td>161.1</td><td>109.8</td><td>-</td><td>33.1</td></tr><tr><td>English</td><td>-</td><td>107.5</td><td>92.1</td><td>37.5</td></tr><tr><td>French</td><td>-</td><td>100.7</td><td>145.3</td><td>41.7</td></tr><tr><td>German</td><td>-</td><td>122.7</td><td>165.1</td><td>46.5</td></tr><tr><td>Italian</td><td>-</td><td>142.7</td><td>155.5</td><td>75.5</td></tr><tr><td>Japanese</td><td>-</td><td>-</td><td>-</td><td>42.4</td></tr><tr><td>Korean</td><td>-</td><td>-</td><td>-</td><td>37.2</td></tr><tr><td>Portuguese</td><td>-</td><td>-</td><td>-</td><td>38.4</td></tr><tr><td>Russian</td><td>-</td><td>200.7</td><td>-</td><td>40.2</td></tr><tr><td>Spanish</td><td>-</td><td>124.7</td><td>108.0</td><td>36.8</td></tr><tr><td>Avg.</td><td>161.1</td><td>129.8</td><td>133.2</td><td>42.9</td></tr><tr><td colspan="5">MFA-Labeled Concat-300s</td></tr><tr><td>Chinese</td><td>1742.4</td><td>235.0</td><td>-</td><td>36.5</td></tr><tr><td>English</td><td>-</td><td>226.7</td><td>227.2</td><td>58.6</td></tr><tr><td>French</td><td>-</td><td>230.6</td><td>2052.2</td><td>53.4</td></tr><tr><td>German</td><td>-</td><td>220.3</td><td>993.4</td><td>62.4</td></tr><tr><td>Italian</td><td>-</td><td>290.5</td><td>5719.4</td><td>81.6</td></tr><tr><td>Japanese</td><td>-</td><td>-</td><td>-</td><td>81.3</td></tr><tr><td>Korean</td><td>-</td><td>-</td><td>-</td><td>42.2</td></tr><tr><td>Portuguese</td><td>-</td><td>-</td><td>-</td><td>50.0</td></tr><tr><td>Russian</td><td>-</td><td>283.3</td><td>-</td><td>43.0</td></tr><tr><td>Spanish</td><td>-</td><td>240.2</td><td>4549.9</td><td>39.6</td></tr><tr><td>Cross-lingual</td><td>-</td><td>-</td><td>-</td><td>34.2</td></tr><tr><td>Avg.</td><td>1742.4</td><td>246.7</td><td>2708.4</td><td>52.9</td></tr><tr><td colspan="5">Human-Labeled</td></tr><tr><td>Raw</td><td>49.9</td><td>88.6</td><td>-</td><td>27.8</td></tr><tr><td>Raw-Noisy</td><td>53.3</td><td>89.5</td><td>-</td><td>41.8</td></tr><tr><td>Concat-60s</td><td>51.1</td><td>86.7</td><td>-</td><td>25.3</td></tr><tr><td>Concat-300s</td><td>410.8</td><td>140.0</td><td>-</td><td>24.8</td></tr><tr><td>Concat-Cross-lingual</td><td>-</td><td>-</td><td>-</td><td>42.5</td></tr><tr><td>Avg.</td><td>141.3</td><td>101.2</td><td>-</td><td>32.4</td></tr></tbody></table>

Table [9](https://arxiv.org/html/2601.21337v1#S4.T9 "Table 9 ‣ 4.6 Precision of Timestamps ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report") reports the AAS of Qwen3-ForcedAligner-0.6B and competing forced-alignment methods on MFA-labeled and human-labeled test sets. Competing methods require language-specific models and support only a limited set of languages, whereas Qwen3-ForcedAligner-0.6B covers multiple languages with a single model and supports cross-lingual, code-switched scenarios. In addition, Qwen3-ForcedAligner-0.6B performs consistently on both short and long utterances, while baseline methods show a sharp degradation in timestamp accuracy on long utterances. Although trained with MFA pseudo-labels, Qwen3-ForcedAligner-0.6B still achieves low AAS on the human-labeled test sets, indicating strong real-world generalization.

5 Conclusion
------------

In this paper we introduce Qwen3-ASR model family, including two ASR models and a forced-alignment model trained with large-scale speech data. By leveraging the strong audio understanding ability of foundation model Qwen3-Omni and our training process with 4 stages, Qwen3-ASR-1.7B and Qwen3-ASR-0.6B outperform competing models of similar or larger size and commercial APIs in both speech coverage and recognition accuracy. They support language identification and speech recognition for 30 languages, conduct accurate ASR under complex acoustic environment, maintain robustness for accents and dialects, and keep performant on singing voice and other speech from real-world scenarios. Besides ASR models, we also propose a novel solution for timestamp prediction and forced alignment: Qwen3-ForcedAligner-0.6B, which is an LLM based NAR timestamp predictor that supports FA for 11 languages and within 5 minutes. It outperforms three mainstream end-to-end ASR model based FA solutions in timestamp accuracy, inference speed and language coverage. Together with the weights of three models, we also open-source a powerful and easy-to-use inference framework. Overall, the Qwen3-ASR family achieves state-of-the-art performance on both real-world evaluations and public benchmarks, and our open-sourced forced-alignment model completes a critical missing piece in the speech technology stack. We hope Qwen3-ASR can contribute to the advancement of speech recognition research and applications, and we will continue to improve our open model family in both accuracy and functional innovations.

6 Authors
---------

Core Contributors: Xian Shi, Xiong Wang, Zhifang Guo, Yongqi Wang, Pei Zhang, Xinyu Zhang, Zishan Guo, Hongkun Hao, Yu Xi, Baosong Yang, Jin Xu†, Jingren Zhou, Junyang Lin†

Contributors: Yunfei Chu, Daren Chen, Ting He, Hangrui Hu, Jiayi Leng, Zheng Li, Yuanjun Lv, Bingshen Mu, Hao Su, Xian Yang, Xuechun Wang, Yuezhang Wang, Zhenglin Wang, Lei Xie, Jianwei Zhang, Xinfa Zhu, Guangdong Zhou

References
----------

*   K. An, Y. Chen, Z. Chen, C. Deng, Z. Du, C. Gao, Z. Gao, B. Gong, X. Li, Y. Li, Y. Liu, X. Lv, Y. Ji, Y. Jiang, B. Ma, H. Luo, C. Ni, Z. Pan, Y. Peng, Z. Peng, P. Wang, H. Wang, H. Wang, W. Wang, W. Wang, Y. Wu, B. Tian, Z. Tan, N. Yang, B. Yuan, J. Ye, J. Yu, Q. Zhang, K. Zou, H. Zhao, S. Zhao, J. Zhou, and Y. Zhu (2025) Fun-ASR Technical Report. External Links: [Link](https://arxiv.org/abs/2509.12508) Cited by: [§4.1](https://arxiv.org/html/2601.21337v1#S4.SS1.p1.1 "4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   R. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Henretty, R. Morais, L. Saunders, F. Tyers, and G. Weber (2020) Common voice: a massively-multilingual speech corpus. In Proceedings of the twelfth language resources and evaluation conference, pp. 4218–4222. Cited by: [item 1](https://arxiv.org/html/2601.21337v1#S4.I1.i1.p1.1 "In 4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   Y. Bai, J. Chen, J. Chen, W. Chen, Z. Chen, C. Ding, L. Dong, Q. Dong, Y. Du, K. Gao, L. Gao, Y. Guo, M. Han, T. Han, W. Hu, X. Hu, Y. Hu, D. Hua, L. Huang, M. Huang, Y. Huang, J. Jin, F. Kong, Z. Lan, T. Li, X. Li, Z. Li, Z. Lin, R. Liu, S. Liu, L. Lu, Y. Lu, J. Ma, S. Ma, Y. Pei, C. Shen, T. Tan, X. Tian, M. Tu, B. Wang, H. Wang, Y. Wang, Y. Wang, H. Xia, R. Xia, S. Xie, H. Xu, M. Yang, B. Zhang, J. Zhang, W. Zhang, Y. Zhang, Y. Zhang, Y. Zheng, and M. Zou (2024) Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition. External Links: [Link](https://arxiv.org/abs/2407.04675) Cited by: [§4.1](https://arxiv.org/html/2601.21337v1#S4.SS1.p1.1 "4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals (2016) Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In Proc. ICASSP, pp. 4960–4964. External Links: [Link](https://doi.org/10.1109/ICASSP.2016.7472621) Cited by: [§1](https://arxiv.org/html/2601.21337v1#S1.p1.1 "1 Introduction ‣ Qwen3-ASR Technical Report").
*   G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. (2025) Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. External Links: [Link](https://arxiv.org/abs/2507.06261) Cited by: [§4.1](https://arxiv.org/html/2601.21337v1#S4.SS1.p1.1 "4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna (2023) Fleurs: few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. 798–805. Cited by: [item 1](https://arxiv.org/html/2601.21337v1#S4.I1.i1.p1.1 "In 4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   Y. Dai, Z. Zhang, S. Wang, L. Li, Z. Guo, T. Zuo, S. Wang, H. Xue, C. Wang, Q. Wang, et al. (2025) Wenetspeech-chuan: a large-scale sichuanese corpus with rich annotation for dialectal speech processing. arXiv preprint arXiv:2509.18004. Cited by: [item 1](https://arxiv.org/html/2601.21337v1#S4.I1.i1.p1.1 "In 4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   A. Graves (2012) Sequence Transduction with Recurrent Neural Networks. External Links: [Link](http://arxiv.org/abs/1211.3711) Cited by: [§1](https://arxiv.org/html/2601.21337v1#S1.p1.1 "1 Introduction ‣ Qwen3-ASR Technical Report").
*   L. Kürzinger, D. Winkelbauer, L. Li, T. Watzel, and G. Rigoll (2020) CTC-Segmentation of Large Corpora for German End-to-End Speech Recognition. In Proc. Speech and Computer, pp. 267–278. External Links: [Link](https://doi.org/10.1007/978-3-030-60276-5%5C_27) Cited by: [§1](https://arxiv.org/html/2601.21337v1#S1.p2.1 "1 Introduction ‣ Qwen3-ASR Technical Report").
*   L. Li, Z. Guo, H. Chen, Y. Dai, Z. Zhang, H. Xue, T. Zuo, C. Wang, S. Wang, J. Li, et al. (2025) Wenetspeech-yue: a large-scale cantonese speech corpus with multi-dimensional annotation. arXiv preprint arXiv:2509.03959. Cited by: [item 1](https://arxiv.org/html/2601.21337v1#S4.I1.i1.p1.1 "In 4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger (2017) Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi. In Proc. Interspeech, pp. 498–502. External Links: [Link](https://doi.org/10.21437/Interspeech.2017-1386) Cited by: [§3.3](https://arxiv.org/html/2601.21337v1#S3.SS3.p1.1 "3.3 Training Strategies ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   B. Mu, P. Guo, Z. Sun, S. Wang, H. Liu, M. Shao, L. Xie, E. S. Chng, L. Xiao, Q. Feng, and D. Wang (2026a) Summary on The Multilingual Conversational Speech Language Model Challenge: Datasets, Tasks, Baselines, and Methods. In Proc. ICASSP, Cited by: [item 3](https://arxiv.org/html/2601.21337v1#S4.I1.i3.p1.1 "In 4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   B. Mu, X. Shi, X. Wang, H. Liu, J. Xu, and L. Xie (2026b) LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech. External Links: [Link](https://arxiv.org/abs/2601.18220) Cited by: [§3.2](https://arxiv.org/html/2601.21337v1#S3.SS2.p2.1 "3.2 Model Design ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   OpenAI (2024) Hello GPT-4o. External Links: [Link](https://openai.com/index/hello-gpt-4o/) Cited by: [§4.1](https://arxiv.org/html/2601.21337v1#S4.SS1.p1.1 "4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   V. Panayotov, G. Chen, D. Povey, and S. Khudanpur (2015) Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5206–5210. Cited by: [item 1](https://arxiv.org/html/2601.21337v1#S4.I1.i1.p1.1 "In 4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever (2023) Robust Speech Recognition via Large-Scale Weak Supervision. In Proc. ICML, pp. 28492–28518. External Links: [Link](https://proceedings.mlr.press/v202/radford23a.html) Cited by: [§1](https://arxiv.org/html/2601.21337v1#S1.p1.1 "1 Introduction ‣ Qwen3-ASR Technical Report"), [§4.1](https://arxiv.org/html/2601.21337v1#S4.SS1.p1.1 "4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   E. Rastorgueva, V. Lavrukhin, and B. Ginsburg (2023) NeMo Forced Aligner and its application to word alignment for subtitle generation. In Proc. Interspeech, pp. 5257–5258. External Links: [Link](https://www.isca-archive.org/interspeech%5C_2023/rastorgueva23%5C_interspeech.html) Cited by: [§1](https://arxiv.org/html/2601.21337v1#S1.p2.1 "1 Introduction ‣ Qwen3-ASR Technical Report").
*   X. Shi, Y. Chen, S. Zhang, and Z. Yan (2023) Achieving timestamp prediction while recognizing with non-autoregressive end-to-end ASR model. In Proc. NCMMSC, Cited by: [§1](https://arxiv.org/html/2601.21337v1#S1.p2.1 "1 Introduction ‣ Qwen3-ASR Technical Report"), [§4.1](https://arxiv.org/html/2601.21337v1#S4.SS1.p5.5 "4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   J. Xu, Z. Guo, H. Hu, Y. Chu, X. Wang, J. He, Y. Wang, X. Shi, T. He, X. Zhu, Y. Lv, Y. Wang, D. Guo, H. Wang, L. Ma, P. Zhang, X. Zhang, H. Hao, Z. Guo, B. Yang, B. Zhang, Z. Ma, X. Wei, S. Bai, K. Chen, X. Liu, P. Wang, M. Yang, D. Liu, X. Ren, B. Zheng, R. Men, F. Zhou, B. Yu, J. Yang, L. Yu, J. Zhou, and J. Lin (2025a) Qwen3-Omni Technical Report. External Links: [Link](https://arxiv.org/abs/2509.17765) Cited by: [item (2)](https://arxiv.org/html/2601.21337v1#S2.I1.i2.p1.1 "In 2.2 Training Strategies ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report"), [§2.1](https://arxiv.org/html/2601.21337v1#S2.SS1.p1.1 "2.1 Architecture ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   J. Xu, Z. Guo, H. Hu, Y. Chu, X. Wang, J. He, Y. Wang, X. Shi, T. He, X. Zhu, Y. Lv, Y. Wang, D. Guo, H. Wang, L. Ma, P. Zhang, X. Zhang, H. Hao, Z. Guo, B. Yang, B. Zhang, Z. Ma, X. Wei, S. Bai, K. Chen, X. Liu, P. Wang, M. Yang, D. Liu, X. Ren, B. Zheng, R. Men, F. Zhou, B. Yu, J. Yang, L. Yu, J. Zhou, and J. Lin (2025b) Qwen3-omni technical report. CoRR abs/2509.17765. Cited by: [§1](https://arxiv.org/html/2601.21337v1#S1.p3.1 "1 Introduction ‣ Qwen3-ASR Technical Report").
*   Z.ai (2025) GLM asr 2512. Note: [https://docs.z.ai/guides/audio/glm-asr-2512](https://docs.z.ai/guides/audio/glm-asr-2512)Accessed: 2026-01-26 Cited by: [§4.1](https://arxiv.org/html/2601.21337v1#S4.SS1.p1.1 "4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   B. Zhang, H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu, X. Chen, C. Zeng, et al. (2022) Wenetspeech: a 10000+ hours multi-domain mandarin corpus for speech recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6182–6186. Cited by: [item 1](https://arxiv.org/html/2601.21337v1#S4.I1.i1.p1.1 "In 4.1 Evaluation Details ‣ 4 Experiments ‣ 3.4 Inference and Usability ‣ 3 Qwen3-ForcedAligner ‣ 2.4 Inference Efficiency ‣ 2.3 Features ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").
*   C. Zheng, S. Liu, M. Li, X. Chen, B. Yu, C. Gao, K. Dang, Y. Liu, R. Men, A. Yang, J. Zhou, and J. Lin (2025) Group Sequence Policy Optimization. External Links: [Link](https://arxiv.org/abs/2507.18071) Cited by: [item (4)](https://arxiv.org/html/2601.21337v1#S2.I1.i4.p1.1 "In 2.2 Training Strategies ‣ 2 Qwen3-ASR ‣ Qwen3-ASR Technical Report").

Appendix
--------

Table A.1: Evaluation on English, Chinese and a range of Chinese dialect benchmarks. As a member of Qwen3-ASR family, Qwen3-ASR-Flash-1208 serves as an API and its results are for reference in the table.

<table><tbody><tr><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td colspan="5">English (en)</td></tr><tr><td></td><td>LibriSpeech</td><td rowspan="2">2.11—4.55</td><td rowspan="2">1.63—3.38</td><td rowspan="2">1.33—2.40</td></tr><tr><td></td><td>clean — other</td></tr><tr><td></td><td>GigaSpeech</td><td>8.88</td><td>8.45</td><td>8.82</td></tr><tr><td></td><td>CV-en</td><td>9.92</td><td>7.39</td><td>6.06</td></tr><tr><td></td><td>Fleurs-en</td><td>4.39</td><td>3.35</td><td>2.72</td></tr><tr><td></td><td>MLS-en</td><td>6.00</td><td>4.58</td><td>3.63</td></tr><tr><td></td><td>Tedlium</td><td>3.85</td><td>4.50</td><td>4.84</td></tr><tr><td></td><td>VoxPopuli</td><td>9.96</td><td>9.15</td><td>8.45</td></tr><tr><td colspan="5">Chinese (zh)</td></tr><tr><td></td><td>WenetSpeech</td><td rowspan="2">5.97—6.88</td><td rowspan="2">4.97—5.88</td><td rowspan="2">4.60—5.80</td></tr><tr><td></td><td>net — meeting</td></tr><tr><td></td><td>AISHELL-2-test</td><td>3.15</td><td>2.71</td><td>2.53</td></tr><tr><td></td><td>SpeechIO</td><td>3.44</td><td>2.88</td><td>2.62</td></tr><tr><td></td><td>Fleurs-zh</td><td>2.88</td><td>2.41</td><td>2.38</td></tr><tr><td></td><td>CV-zh</td><td>6.89</td><td>5.35</td><td>4.45</td></tr><tr><td colspan="5">Chinese Dialect</td></tr><tr><td></td><td>KeSpeech</td><td>7.08</td><td>5.10</td><td>3.28</td></tr><tr><td></td><td>Fleurs-yue</td><td>5.79</td><td>3.98</td><td>3.50</td></tr><tr><td></td><td>CV-yue</td><td>9.50</td><td>7.57</td><td>4.86</td></tr><tr><td></td><td>CV-zh-tw</td><td>5.59</td><td>3.77</td><td>3.30</td></tr><tr><td></td><td>WenetSpeech-Yue</td><td rowspan="2">7.54—9.92</td><td rowspan="2">5.82—8.85</td><td rowspan="2">5.84—8.20</td></tr><tr><td></td><td>short — long</td></tr><tr><td></td><td>WenetSpeech-Chuan</td><td rowspan="2">13.92—24.45</td><td rowspan="2">11.99—21.63</td><td rowspan="2">11.52—20.82</td></tr><tr><td></td><td>easy — hard</td></tr></tbody></table>

Table A.2: Evaluation of Qwen3-ASR on open-source multilingual benchmarks. As a member of Qwen3-ASR family, Qwen3-ASR-Flash-1208 serves as an API and its results are for reference in the table. (a) MLS, CommonVoice, and MLC-SLM.

<table><tbody><tr><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td colspan="5">MLS</td></tr><tr><td></td><td>da</td><td>16.79</td><td>11.73</td><td>7.58</td></tr><tr><td></td><td>de</td><td>9.52</td><td>6.05</td><td>4.11</td></tr><tr><td></td><td>en</td><td>6.04</td><td>4.58</td><td>3.63</td></tr><tr><td></td><td>es</td><td>7.19</td><td>4.63</td><td>3.29</td></tr><tr><td></td><td>fr</td><td>8.55</td><td>5.26</td><td>3.16</td></tr><tr><td></td><td>it</td><td>19.21</td><td>13.20</td><td>7.88</td></tr><tr><td></td><td>pl</td><td>26.09</td><td>15.26</td><td>9.76</td></tr><tr><td></td><td>pt</td><td>12.16</td><td>7.71</td><td>6.83</td></tr><tr><td colspan="5">CommonVoice</td></tr><tr><td></td><td>ar</td><td>45.99</td><td>37.97</td><td>33.86</td></tr><tr><td></td><td>de</td><td>9.44</td><td>5.85</td><td>3.53</td></tr><tr><td></td><td>en</td><td>9.92</td><td>7.39</td><td>6.06</td></tr><tr><td></td><td>es</td><td>7.16</td><td>4.65</td><td>3.14</td></tr><tr><td></td><td>fr</td><td>12.25</td><td>8.56</td><td>5.88</td></tr><tr><td></td><td>it</td><td>10.16</td><td>5.40</td><td>3.21</td></tr><tr><td></td><td>ja</td><td>14.96</td><td>11.64</td><td>9.31</td></tr><tr><td></td><td>ko</td><td>8.48</td><td>5.88</td><td>3.82</td></tr><tr><td></td><td>pt</td><td>11.30</td><td>7.10</td><td>5.42</td></tr><tr><td></td><td>ru</td><td>14.07</td><td>8.28</td><td>5.73</td></tr><tr><td></td><td>yue</td><td>9.50</td><td>7.57</td><td>4.86</td></tr><tr><td></td><td>zh</td><td>6.89</td><td>5.35</td><td>4.45</td></tr><tr><td></td><td>zh_tw</td><td>5.59</td><td>3.77</td><td>3.30</td></tr><tr><td colspan="5">MLC-SLM</td></tr><tr><td></td><td>de</td><td>19.78</td><td>17.19</td><td>15.76</td></tr><tr><td></td><td>en</td><td>7.44</td><td>6.41</td><td>6.55</td></tr><tr><td></td><td>es</td><td>13.89</td><td>11.07</td><td>9.31</td></tr><tr><td></td><td>fr</td><td>22.96</td><td>20.75</td><td>22.98</td></tr><tr><td></td><td>it</td><td>21.31</td><td>16.75</td><td>14.93</td></tr><tr><td></td><td>jp</td><td>14.74</td><td>11.80</td><td>9.74</td></tr><tr><td></td><td>ko</td><td>10.31</td><td>8.61</td><td>8.09</td></tr><tr><td></td><td>pt</td><td>34.97</td><td>26.64</td><td>28.14</td></tr><tr><td></td><td>ru</td><td>19.24</td><td>15.17</td><td>13.16</td></tr><tr><td></td><td>th</td><td>19.51</td><td>14.34</td><td>19.66</td></tr><tr><td></td><td>vi</td><td>17.67</td><td>14.92</td><td>13.11</td></tr></tbody></table>

(b) Fleurs.

<table><tbody><tr><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td colspan="5">Fleurs</td></tr><tr><td></td><td>ar</td><td>25.51</td><td>16.98</td><td>14.78</td></tr><tr><td></td><td>cs</td><td>47.67</td><td>22.42</td><td>18.68</td></tr><tr><td></td><td>da</td><td>36.36</td><td>21.00</td><td>11.85</td></tr><tr><td></td><td>de</td><td>6.48</td><td>3.92</td><td>3.03</td></tr><tr><td></td><td>el</td><td>49.67</td><td>28.08</td><td>13.85</td></tr><tr><td></td><td>en</td><td>4.39</td><td>3.35</td><td>2.72</td></tr><tr><td></td><td>es</td><td>4.94</td><td>3.36</td><td>2.68</td></tr><tr><td></td><td>fa</td><td>53.76</td><td>29.90</td><td>18.37</td></tr><tr><td></td><td>fi</td><td>46.59</td><td>25.23</td><td>12.21</td></tr><tr><td></td><td>fil</td><td>36.10</td><td>24.29</td><td>19.17</td></tr><tr><td></td><td>fr</td><td>7.72</td><td>4.75</td><td>3.44</td></tr><tr><td></td><td>hi</td><td>19.12</td><td>17.15</td><td>13.77</td></tr><tr><td></td><td>hu</td><td>59.47</td><td>34.22</td><td>21.77</td></tr><tr><td></td><td>id</td><td>7.92</td><td>5.16</td><td>3.65</td></tr><tr><td></td><td>it</td><td>4.99</td><td>2.41</td><td>1.60</td></tr><tr><td></td><td>ja</td><td>8.33</td><td>5.20</td><td>3.09</td></tr><tr><td></td><td>ko</td><td>3.72</td><td>2.57</td><td>2.07</td></tr><tr><td></td><td>mk</td><td>37.26</td><td>19.05</td><td>–</td></tr><tr><td></td><td>ms</td><td>17.66</td><td>10.39</td><td>11.37</td></tr><tr><td></td><td>nl</td><td>14.02</td><td>7.04</td><td>4.35</td></tr><tr><td></td><td>pl</td><td>24.71</td><td>12.54</td><td>7.24</td></tr><tr><td></td><td>pt</td><td>6.21</td><td>3.92</td><td>3.18</td></tr><tr><td></td><td>ro</td><td>44.26</td><td>20.70</td><td>10.45</td></tr><tr><td></td><td>ru</td><td>9.91</td><td>5.99</td><td>4.81</td></tr><tr><td></td><td>sv</td><td>35.87</td><td>19.36</td><td>15.02</td></tr><tr><td></td><td>th</td><td>8.34</td><td>6.32</td><td>5.53</td></tr><tr><td></td><td>tr</td><td>16.18</td><td>9.47</td><td>6.13</td></tr><tr><td></td><td>vi</td><td>8.52</td><td>5.55</td><td>3.64</td></tr><tr><td></td><td>yue</td><td>5.79</td><td>3.98</td><td>3.50</td></tr><tr><td></td><td>zh</td><td>2.88</td><td>2.41</td><td>2.38</td></tr></tbody></table>