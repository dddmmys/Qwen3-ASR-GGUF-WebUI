# coding=utf-8
import os
import time
import re
import codecs
import dataclasses
import numpy as np
import multiprocessing as mp
from pathlib import Path
from collections import deque
from typing import Optional, List

from .schema import MsgType, StreamingMessage, DecodeResult, ASREngineConfig, TranscribeResult, ForcedAlignItem, ForcedAlignResult
from .asr_worker import asr_helper_worker_proc
from .utils import normalize_language_name, validate_language
from . import llama

@dataclasses.dataclass
class ASRS_Segment:
    """ç®¡ç†åˆ†ç‰‡è®°å¿†åŠå…¶ç‰©ç†æ—¶é—´åæ ‡"""
    idx: int
    audio_start: float
    audio_end: float
    text: str = ""
    items: List[ForcedAlignItem] = None   

class QwenASREngine:
    """Qwen3-ASR æµå¼è½¬å½•å¼•æ“ (GGUF åç«¯) - ç»Ÿä¸€è¾…åŠ©è¿›ç¨‹æ¶æ„"""
    def __init__(self, config: ASREngineConfig):
        self.verbose = config.verbose
        if self.verbose: print(f"--- [QwenASR] åˆå§‹åŒ–å¼•æ“ (DML: {config.use_dml}) ---")

        from qwen_asr_gguf.inference import llama
        self.llama_mod = llama # keep reference
        
        # è·¯å¾„è§£æ
        llm_gguf = os.path.join(config.model_dir, config.llm_fn)

        
        # 1. å¯åŠ¨è¾…åŠ©å­è¿›ç¨‹ (ç¼–ç  + å¯¹é½)
        self.to_worker_q = mp.Queue()
        self.from_enc_q = mp.Queue()
        self.from_align_q = mp.Queue()
        
        self.helper_proc = mp.Process(
            target=asr_helper_worker_proc, 
            args=(self.to_worker_q, self.from_enc_q, self.from_align_q, config), 
            daemon=True
        )
        self.helper_proc.start()
        
        # 2. åŠ è½½è¯†åˆ« LLM
        self.model = llama.LlamaModel(llm_gguf)
        self.embedding_table = llama.get_token_embeddings_gguf(llm_gguf)
        self.ctx = llama.LlamaContext(self.model, n_ctx=config.n_ctx, n_batch=4096, embeddings=False)
        
        # 3. ç­‰å¾…å­è¿›ç¨‹å°±ç»ªä¿¡å· (åŒ…å« Encoder é¢„çƒ­å®Œæˆ)
        msg = self.from_enc_q.get()
        if msg.msg_type == MsgType.MSG_READY and self.verbose:
            print("--- [QwenASR] è¾…åŠ©è¿›ç¨‹å·²å°±ç»ª ---")

        # ç¼“å­˜ Token ID
        self.ID_IM_START = self.model.token_to_id("<|im_start|>")
        self.ID_IM_END = self.model.token_to_id("<|im_end|>")
        self.ID_AUDIO_START = self.model.token_to_id("<|audio_start|>")
        self.ID_AUDIO_END = self.model.token_to_id("<|audio_end|>")
        self.ID_ASR_TEXT = self.model.token_to_id("<asr_text>")

    def shutdown(self):
        # å‘è¾…åŠ©è¿›ç¨‹å‘é€åœæ­¢ä¿¡å·
        if self.helper_proc:
            self.to_worker_q.put(StreamingMessage(MsgType.CMD_STOP))
            self.helper_proc.join()
        if self.verbose: print("--- [QwenASR] å¼•æ“å·²å…³é—­ ---")

    def _build_prompt_embd(self, audio_embd: np.ndarray, prefix_text: str, context: Optional[str], language: Optional[str]):
        """æ„é€ ç”¨äº LLM è¾“å…¥çš„ Embedding åºåˆ— (åŒºå—åŒ–æ‰“åŒ…æ¨¡å¼)"""
        def tk(t): return self.model.tokenize(t)

        # 1. åŒºå— A: éŸ³é¢‘ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ (System + User Header)
        prefix_str = f"system\n{context or 'You are a helpful assistant.'}"
        prefix_tokens = [self.ID_IM_START] + tk(prefix_str) + [self.ID_IM_END] + \
                        [self.ID_IM_START] + tk("user\n") + [self.ID_AUDIO_START]
        
        # 2. åŒºå— B: éŸ³é¢‘ä¹‹åçš„æ‰€æœ‰å†…å®¹ (Instruction + Assistant Header + History)
        suffix_head = f"assistant\n"
        if language: suffix_head += f"language {language}"
        
        suffix_tokens = [self.ID_AUDIO_END] + [self.ID_IM_END] + \
                        [self.ID_IM_START] + tk(suffix_head) + [self.ID_ASR_TEXT] + tk(prefix_text)

        # 3. ç»Ÿè®¡å¹¶æ‹¼æ¥
        n_pre, n_aud, n_suf = len(prefix_tokens), audio_embd.shape[0], len(suffix_tokens)
        total_embd = np.zeros((n_pre + n_aud + n_suf, self.model.n_embd), dtype=np.float32)
        
        total_embd[:n_pre] = self.embedding_table[prefix_tokens]
        total_embd[n_pre : n_pre + n_aud] = audio_embd
        total_embd[n_pre + n_aud:] = self.embedding_table[suffix_tokens]
        
        return total_embd

    def _decode(
        self, 
        full_embd: np.ndarray,
        prefix_text: str, 
        rollback_num: int,
        is_last_chunk: bool = False, 
        temperature: float = 0.4
    ) -> DecodeResult:
        """åº•å±‚æ–¹æ³•ï¼šæ‰§è¡Œå•æ¬¡ LLM ç”Ÿæˆå¾ªç¯ï¼ˆç‰©ç†æ¨ç†ï¼‰"""
        result = DecodeResult()
        
        total_len = full_embd.shape[0]
        pos_base = np.arange(0, total_len, dtype=np.int32)
        pos_arr = np.concatenate([pos_base, pos_base, pos_base, np.zeros(total_len, dtype=np.int32)])
        batch = self.llama_mod.LlamaBatch(max(total_len * 4, 8192), self.model.n_embd, 1)
        batch.set_embd(full_embd, pos=pos_arr)
        
        # 1. Prefill
        self.ctx.clear_kv_cache()
        t_pre_start = time.time()
        self.ctx.decode(batch)
        prefill_time = time.time() - t_pre_start
        
        # 2. Generation Loopï¼ˆä½¿ç”¨æ–°é‡‡æ ·å™¨å’Œéšæœºç§å­ï¼‰
        t_gen_start = time.time()
        n_gen_tokens = 0
        display_queue = deque()
        stable_tokens = []
        stable_text_acc = ""
        cur_pos = total_len
        gen_batch = self.llama_mod.LlamaBatch(4, 0, 1)
        text_decoder = codecs.getincrementaldecoder('utf-8')(errors='replace')
        
        # æ¯æ¬¡è§£ç ä½¿ç”¨æ–°çš„éšæœºç§å­
        seed = int(np.random.randint(0, 2**31 - 1))
        sampler = self.llama_mod.LlamaSampler(temperature=temperature, seed=seed)
        last_sampled_token = sampler.sample(self.ctx.ptr)
        for _ in range(512): # Max new tokens per chunk
            if last_sampled_token in [self.model.eos_token, self.ID_IM_END]:
                break
            
            gen_batch.set_token(last_sampled_token, pos=np.array([cur_pos, cur_pos, cur_pos, 0], dtype=np.int32))
            self.ctx.decode(gen_batch)
            
            display_queue.append(last_sampled_token)
            if len(display_queue) > rollback_num:
                ready_token = display_queue.popleft()
                stable_tokens.append(ready_token)
                piece = text_decoder.decode(self.model.token_to_bytes(ready_token))
                if piece:
                    print(re.sub('([ï¼Œã€‚ï¼Ÿï¼])', '\\1\n', piece), end='', flush=True)
                    stable_text_acc += piece
            
            # ç†”æ–­æ£€æŸ¥ï¼šæ£€æµ‹é‡å¤å¾ªç¯
            if len(stable_tokens) > 15:
                if len(set(stable_tokens[-15:])) <= 3:
                    result.is_aborted = True
                    break
            
            cur_pos += 1
            last_sampled_token = sampler.sample(self.ctx.ptr)
            n_gen_tokens += 1
            
        gen_time = time.time() - t_gen_start
        del sampler  # é‡Šæ”¾é‡‡æ ·å™¨èµ„æº
        del batch
            
        if is_last_chunk and not result.is_aborted:
            while display_queue:
                t = display_queue.popleft()
                stable_tokens.append(t)
                piece = text_decoder.decode(self.model.token_to_bytes(t))
                if piece:
                    print(re.sub('([ï¼Œã€‚ï¼Ÿï¼])', '\\1\n', piece), end="", flush=True)
                    stable_text_acc += piece
            final_p = text_decoder.decode(b"", final=True)
            if final_p: 
                print(final_p, end='', flush=True)
                stable_text_acc += final_p
        
        # å¡«å……ç»“æœï¼ˆå†…æ ¸è¾“å‡ºæ ‡å‡†åŒ–ï¼‰
        result.text = stable_text_acc
        result.stable_tokens = stable_tokens
        result.t_prefill = prefill_time
        result.t_generate = gen_time
        result.n_prefill = total_len
        result.n_generate = n_gen_tokens
        result.n_generate = n_gen_tokens
        return result

    def _safe_decode(
        self, 
        full_embd: np.ndarray, 
        prefix_text: str, 
        rollback_num: int, 
        is_last_chunk: bool, 
        temperature: float
    ) -> DecodeResult:
        """å¸¦ç†”æ–­åŠ æ¸©é‡è¯•çš„é«˜å±‚æ¨ç†å°è£…"""
        for i in range(4):
            res = self._decode(full_embd, prefix_text, rollback_num, is_last_chunk, temperature)
            if not res.is_aborted:
                break
            temperature += 0.3
            res.text += "====è§£ç æœ‰è¯¯ï¼Œå¼ºåˆ¶ç†”æ–­===="
            print(f"\n\n[!] è§¦å‘é‡è¯• (Temp -> {temperature:.1f})\n")
        return res 

    def _collect_alignment(
        self, 
        idx: int, 
        all_segments: List[ASRS_Segment], 
        all_aligned_items: List[ForcedAlignItem], 
        stats: dict
    ):
        """åŒæ­¥å›æ”¶æŒ‡å®šç´¢å¼•çš„å¯¹é½ç»“æœå¹¶æ›´æ–°çŠ¶æ€"""
        if idx < 0 or idx >= len(all_segments): return
        
        align_msg = self.from_align_q.get()
        if align_msg.msg_type == MsgType.MSG_ALIGN and align_msg.data:
            ares: ForcedAlignResult = align_msg.data
            all_segments[idx].items = ares.items
            all_aligned_items.extend(ares.items)
            if ares.performance:
                stats["align_enc_time"] += ares.performance.get("encoder_time", 0)
                stats["align_dec_time"] += ares.performance.get("decoder_time", 0)

    def _print_stats(self, stats: dict, audio_duration: float, t_total: float):
        """æ‰“å°è½¬å½•è¿‡ç¨‹çš„æ€§èƒ½ç»Ÿè®¡æŒ‡æ ‡"""
        rtf = t_total / audio_duration if audio_duration > 0 else 0
        pre_speed = stats["prefill_tokens"] / stats["prefill_time"] if stats["prefill_time"] > 0 else 0
        gen_speed = stats["decode_tokens"] / stats["decode_time"] if stats["decode_time"] > 0 else 0
        
        print(f"\n\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  ğŸ”¹ RTF (å®æ—¶ç‡) : {rtf:.3f} (è¶Šå°è¶Šå¿«)")
        print(f"  ğŸ”¹ éŸ³é¢‘æ—¶é•¿    : {audio_duration:.2f} ç§’")
        print(f"  ğŸ”¹ æ€»å¤„ç†è€—æ—¶  : {t_total:.2f} ç§’")
        print(f"  ğŸ”¹ ç¼–ç ç­‰å¾…    : {stats['wait_time']:.2f} ç§’")
        print(f"  ğŸ”¹ å¯¹é½æ€»æ—¶    : {stats['align_enc_time']+stats['align_dec_time']:.2f} ç§’ (åˆ†æ®µå¼‚æ­¥å¯¹é½)")
        print(f"  ğŸ”¹ LLM é¢„å¡«å……  : {stats['prefill_time']:.3f} ç§’ ({stats['prefill_tokens']} tokens, {pre_speed:.1f} tokens/s)")
        print(f"  ğŸ”¹ LLM ç”Ÿæˆ    : {stats['decode_time']:.3f} ç§’ ({stats['decode_tokens']} tokens, {gen_speed:.1f} tokens/s)")

    def transcribe(
        self, 
        audio_file: str, 
        language: Optional[str] = None, 
        context: Optional[str] = None, 
        chunk_size: float = 40.0,
        start_second: float = 0.0,
        duration: float = 0.0,
        temperature: float = 0.4,
        memory_num: int = 1,
        rollback_num: int = 5
    ) -> TranscribeResult:
        """è¿è¡Œå®Œæ•´è½¬å½•æµæ°´çº¿ (ä»æ–‡ä»¶åŠ è½½éŸ³é¢‘)"""
        from .utils import load_audio
        audio = load_audio(audio_file, start_second=start_second, duration=duration)
        
        return self.asr(
            audio=audio,
            context=context or "",
            language=language,
            chunk_size_sec=chunk_size,
            memory_chunks=memory_num,
            temperature=temperature,
            rollback_num=rollback_num
        )

    def asr(
        self, 
        audio: np.ndarray,
        context: Optional[str],
        language: Optional[str],
        chunk_size_sec: float = 40.0,
        memory_chunks: int = 2,
        temperature: float = 0.4,
        rollback_num: int = 5
    ) -> TranscribeResult:
        """è¿è¡Œå®Œæ•´è½¬å½•æµæ°´çº¿ (ä¸‰çº§æµæ°´çº¿ï¼ši+1 é¢„å–, i è¯†åˆ«, i-1 å¯¹é½)"""
        # è¯­è¨€å½’ä¸€åŒ–ä¸æ ¡éªŒ
        if language:
            language = normalize_language_name(language)
            validate_language(language)

        sr = 16000
        samples_per_chunk = int(chunk_size_sec * sr)
        total_len = len(audio)
        num_chunks = int(np.ceil(total_len / samples_per_chunk))
        total_duration = total_len / sr
        
        # è®°å¿†ç®¡ç† (é¢„å®šä¹‰æ‰€æœ‰åˆ†ç‰‡çš„ç‰©ç†è¾¹ç•Œ)
        all_segments: List[ASRS_Segment] = [
            ASRS_Segment(
                idx=i,
                audio_start=i * chunk_size_sec,
                audio_end=min((i + 1) * chunk_size_sec, total_duration)
            ) for i in range(num_chunks)
        ]
        asr_memory = deque(maxlen=memory_chunks) # å­˜å‚¨ (embd, text)
        total_full_text = ""
        all_aligned_items: List[ForcedAlignItem] = []
        
        # ç»Ÿè®¡æŒ‡æ ‡
        stats = {
            "prefill_time": 0.0, "decode_time": 0.0,
            "prefill_tokens": 0, "decode_tokens": 0,
            "wait_time": 0.0, "encode_time": 0.0,
            "align_enc_time": 0.0, "align_dec_time": 0.0
        }
        t_main_start = time.time()

        # å‘é€ç¼–ç ä»»åŠ¡
        def send_enc(idx):
            if idx >= num_chunks: return
            s, e = idx * samples_per_chunk, min((idx + 1) * samples_per_chunk, total_len)
            data = audio[s:e]
            if len(data) < samples_per_chunk: 
                data = np.pad(data, (0, samples_per_chunk - len(data)))
            self.to_worker_q.put(StreamingMessage(MsgType.CMD_ENCODE, data=data, is_last=(idx == num_chunks - 1)))

        # å‘é€å¯¹é½ä»»åŠ¡
        def send_align(idx):
            if idx < 0 or idx >= len(all_segments): return
            seg = all_segments[idx]
            if not seg.text.strip():
                # æ— æ–‡æœ¬æ—¶å‘é€ç©ºç»“æœå ä½ï¼Œä¿è¯æ¶ˆæ¯é˜Ÿåˆ—è®¡æ•°æ­£ç¡®
                self.to_worker_q.put(StreamingMessage(MsgType.CMD_ALIGN, data=None, text="", is_last=(idx == num_chunks-1)))
                return

            # å¯¹é½ç‰©ç†èµ·ç‚¹ï¼šé€‰å–â€œä¸Šä¸€ç‰‡æœ€åä¸€ä¸ªå­—çš„ç»“å°¾â€å’Œâ€œåˆ†ç‰‡ç‰©ç†èµ·è·‘ç‚¹å‰ 10sâ€çš„è¾ƒå¤§å€¼
            offset_sec = seg.audio_start
            if idx > 0 and all_segments[idx-1].items:
                last_end = all_segments[idx-1].items[-1].end_time
                prev_limit = all_segments[idx-1].audio_end 
                offset_sec = min(prev_limit, max(last_end, prev_limit - 10.0))
            
            # å¯¹é½éŸ³é¢‘æˆªå–ï¼šä» offset åˆ°æœ¬ç‰‡ç‰©ç†ç»“å°¾
            s_smpl, e_smpl = int(offset_sec * sr), int(seg.audio_end * sr)
            audio_slice = audio[s_smpl:e_smpl]
            
            self.to_worker_q.put(StreamingMessage(
                msg_type=MsgType.CMD_ALIGN,
                data=audio_slice,
                text=seg.text,
                offset_sec=float(offset_sec),
                language=language,
                is_last=(idx == num_chunks - 1)
            ))

        # --- ä¸‰çº§æµæ°´çº¿ä¸»å¾ªç¯ ---
        if num_chunks > 0: send_enc(0)

        for i in range(num_chunks):
            # 1. æ‹¿åˆ°ç¬¬ i ç‰‡æ®µéŸ³é¢‘ç‰¹å¾
            t_w_start = time.time()
            msg = self.from_enc_q.get()
            stats["wait_time"] += (time.time() - t_w_start)
            stats["encode_time"] += msg.encode_time
            audio_feature, was_last = msg.data, msg.is_last
            
            # 2. æ‹¿åˆ° i-2 ç‰‡æ®µæ—¶é—´æˆ³ (ç”¨ä»¥é©±åŠ¨ i-1 çš„å¯¹é½èµ·ç‚¹)
            if i >= 2: self._collect_alignment(i - 2, all_segments, all_aligned_items, stats)
            
            # 3. è§¦å‘ i+1 ç‰¹å¾æå–
            if not was_last: send_enc(i + 1)
            
            # 4. è§¦å‘ i-1 æ—¶é—´æˆ³åŒ¹é…
            if i >= 1: send_align(i - 1)
            
            # 5. è¯†åˆ«ç¬¬ i ç‰‡æ®µæ–‡å­—
            prefix_text = "".join([m[1] for m in asr_memory])
            combined_audio = np.concatenate([m[0] for m in asr_memory] + [audio_feature], axis=0)
            full_embd = self._build_prompt_embd(combined_audio, prefix_text, context, language)
            
            # å¸¦ç†”æ–­åŠ æ¸©é‡è¯•çš„è§£ç è°ƒç”¨
            res = self._safe_decode(full_embd, prefix_text, rollback_num, was_last, temperature)

            
            # è®°å¿†ç®¡ç†
            all_segments[i].text = res.text
            asr_memory.append((audio_feature, res.text))
            
            total_full_text += res.text
            stats["prefill_tokens"] += res.n_prefill; stats["prefill_time"] += res.t_prefill
            stats["decode_tokens"] += res.n_generate; stats["decode_time"] += res.t_generate

        # --- æ”¶å°¾é€»è¾‘ ---
        if num_chunks >= 2: 
            self._collect_alignment(num_chunks - 2, all_segments, all_aligned_items, stats)
            
        if num_chunks >= 1:
            send_align(num_chunks - 1)
            self._collect_alignment(num_chunks - 1, all_segments, all_aligned_items, stats)

        # 4. ç»“æœæ•´ç†
        all_aligned_items.sort(key=lambda x: x.start_time)
        t_total = time.time() - t_main_start
        if self.verbose: self._print_stats(stats, total_duration, t_total)
            
        return TranscribeResult(
            text=total_full_text,
            alignment=ForcedAlignResult(items=all_aligned_items) if all_aligned_items else None,
            performance=stats
        )
