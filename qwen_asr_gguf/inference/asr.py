# coding=utf-8
import os
import time
import re
import codecs
import numpy as np
import multiprocessing as mp
from pathlib import Path
from collections import deque
from typing import Optional, List

from .schema import MsgType, StreamingMessage, DecodeResult
from .encoder import encoder_worker_proc
from . import llama

class QwenASREngine:
    """Qwen3-ASR æµå¼è½¬å½•å¼•æ“ (GGUF åç«¯)"""
    def __init__(
        self,
        encoder_onnx: str,
        llm_gguf: str,
        mel_filters: str,
        n_ctx: int = 4096,
        verbose: bool = True,
        use_dml: bool = True
    ):
        self.verbose = verbose
        if verbose: print(f"--- [QwenASR] åˆå§‹åŒ–å¼•æ“ ---")
        
        # 1. åŠ è½½ LLM
        self.model = llama.LlamaModel(llm_gguf)
        self.embedding_table = llama.get_token_embeddings_gguf(llm_gguf)
        self.ctx = llama.LlamaContext(self.model, n_ctx=n_ctx, n_batch=4096, embeddings=False)
        
        # 2. å¯åŠ¨éŸ³é¢‘ç¼–ç å™¨å­è¿›ç¨‹
        self.to_enc_q = mp.Queue()
        self.from_enc_q = mp.Queue()
        self.enc_proc = mp.Process(
            target=encoder_worker_proc, 
            args=(self.to_enc_q, self.from_enc_q, encoder_onnx, mel_filters, 40.0, use_dml), 
            daemon=True
        )
        self.enc_proc.start()
        
        # ç­‰å¾…ç¼–ç å™¨é¢„çƒ­åŠå°±ç»ªä¿¡å·
        msg = self.from_enc_q.get()
        if msg.msg_type == MsgType.MSG_READY and verbose:
            print("--- [QwenASR] ç¼–ç å™¨å·²å°±ç»ª ---")

        # ç¼“å­˜ Token ID
        self.ID_IM_START = self.model.token_to_id("<|im_start|>")
        self.ID_IM_END = self.model.token_to_id("<|im_end|>")
        self.ID_AUDIO_START = self.model.token_to_id("<|audio_start|>")
        self.ID_AUDIO_END = self.model.token_to_id("<|audio_end|>")
        self.ID_ASR_TEXT = self.model.token_to_id("<asr_text>")

    def shutdown(self):
        self.to_enc_q.put(StreamingMessage(MsgType.CMD_STOP))
        self.enc_proc.join()
        if self.verbose: print("--- [QwenASR] å¼•æ“å·²å…³é—­ ---")

    def _build_prompt_embd(self, audio_embd: np.ndarray, prefix_text: str, context: Optional[str], language: Optional[str]):
        """æ„é€ ç”¨äº LLM è¾“å…¥çš„ Embedding åºåˆ—"""
        system_text = "You are a helpful assistant."
        user_prompt = f"{context}\n\n" if context else ""
        assistant_prompt = "assistant\n"
        if language: assistant_prompt += f"language {language}"

        def tk(t): return self.model.tokenize(t)

        prefix_tokens = [self.ID_IM_START] + tk(f"system\n{system_text}") + [self.ID_IM_END] + \
                        [self.ID_IM_START] + tk(f"user\n{user_prompt}") + [self.ID_AUDIO_START]
        
        suffix_tokens = [self.ID_AUDIO_END] + tk("æ•°å­—ç”¨0123456789ï¼Œè¯­éŸ³è½¬å½•ï¼š") + [self.ID_IM_END] + \
                        [self.ID_IM_START] + tk(assistant_prompt) + [self.ID_ASR_TEXT] + \
                        tk(prefix_text)

        n_pre, n_aud, n_suf = len(prefix_tokens), audio_embd.shape[0], len(suffix_tokens)
        total_embd = np.zeros((n_pre + n_aud + n_suf, self.model.n_embd), dtype=np.float32)
        total_embd[:n_pre] = self.embedding_table[prefix_tokens]
        total_embd[n_pre : n_pre + n_aud] = audio_embd
        total_embd[n_pre + n_aud:] = self.embedding_table[suffix_tokens]
        
        return total_embd

    def _run_llm_buffered(
        self, 
        full_embd: np.ndarray,
        prefix_text: str, 
        rollback_num: int,
        is_last_chunk: bool = False, 
        temperature: float = 0.4
    ) -> DecodeResult:
        """æ ¸å¿ƒæ¨ç†å¾ªç¯ï¼šå®ç°äº†å›æ»šç¼“å†²åŒºã€å®æ—¶æµå¼è¾“å‡ºå’Œé‡å¤ç†”æ–­æœºåˆ¶"""
        res = DecodeResult()
        t_len = full_embd.shape[0]
        
        # 1. Prefill
        pos_base = np.arange(0, t_len, dtype=np.int32)
        pos_arr = np.concatenate([pos_base, pos_base, pos_base, np.zeros(t_len, dtype=np.int32)])
        batch = llama.LlamaBatch(max(t_len * 4, 8192), self.model.n_embd, 1)
        batch.set_embd(full_embd, pos=pos_arr)
        
        self.ctx.clear_kv_cache()
        t0 = time.time()
        self.ctx.decode(batch)
        res.t_prefill = (time.time() - t0) * 1000
        res.n_prefill = t_len

        # 2. Generation Loop
        t1 = time.time()
        n_gen = 0
        display_queue = deque()
        stable_text_acc = ""
        cur_pos = t_len
        gen_batch = llama.LlamaBatch(4, 0, 1)
        utf8_decoder = codecs.getincrementaldecoder('utf-8')(errors='replace')
        
        # é‡‡æ ·åˆå§‹åŒ–
        seed = int(np.random.randint(0, 2**31 - 1))
        sampler = llama.LlamaSampler(temperature=temperature, seed=seed)
        token = sampler.sample(self.ctx.ptr)
        
        for _ in range(150): # Max new tokens per chunk
            if token in [self.model.eos_token, self.ID_IM_END]: break
            
            gen_batch.set_token(token, pos=np.array([cur_pos, cur_pos, cur_pos, 0], dtype=np.int32))
            self.ctx.decode(gen_batch)
            
            display_queue.append(token)
            if len(display_queue) > rollback_num:
                ready_token = display_queue.popleft()
                res.stable_tokens.append(ready_token)
                piece = utf8_decoder.decode(self.model.token_to_bytes(ready_token))
                if piece:
                    # ä½¿ç”¨æ­£åˆ™å¤„ç†æ¢è¡Œè¾“å‡º
                    print(re.sub('([ï¼Œã€‚ï¼Ÿï¼])', '\\1\n', piece), end='', flush=True)
                    stable_text_acc += piece
            
            # ç†”æ–­æ£€æµ‹
            if len(res.stable_tokens) > 30 and len(set(res.stable_tokens[-30:])) <= 3:
                res.is_aborted = True; break
            
            cur_pos += 1
            n_gen += 1
            token = sampler.sample(self.ctx.ptr)
            
        # æœ€åä¸€å—éŸ³é¢‘çš„å¼ºåˆ¶åˆ·æ–°
        if is_last_chunk and not res.is_aborted:
            while display_queue:
                t = display_queue.popleft()
                res.stable_tokens.append(t)
                piece = utf8_decoder.decode(self.model.token_to_bytes(t))
                if piece:
                    print(re.sub('([ï¼Œã€‚ï¼Ÿï¼])', '\\1\n', piece), end='', flush=True)
                    stable_text_acc += piece
            final_p = utf8_decoder.decode(b"", final=True)
            if final_p:
                print(final_p, end='', flush=True)
                stable_text_acc += final_p
        
        res.t_generate = (time.time() - t1) * 1000
        res.n_generate = n_gen
        res.text = prefix_text + stable_text_acc
        res.new_text = stable_text_acc
        return res

    def transcribe(
        self, 
        audio: np.ndarray,
        context: str = "",
        language: str = "Chinese",
        chunk_size_sec: float = 40.0,
        memory_chunks: int = 2,
        temperature: float = 0.4,
        rollback_num: int = 5
    ) -> str:
        """è¿è¡Œå®Œæ•´è½¬å½•æµæ°´çº¿"""
        sr = 16000
        samples_per_chunk = int(chunk_size_sec * sr)
        total_len = len(audio)
        num_chunks = int(np.ceil(total_len / samples_per_chunk))
        
        history_segments = deque(maxlen=memory_chunks)
        total_full_text = ""
        
        # ç»Ÿè®¡æŒ‡æ ‡ (ä¸ 21 è„šæœ¬ä¸€è‡´)
        stats = {
            "prefill_time": 0.0, "decode_time": 0.0,
            "prefill_tokens": 0, "decode_tokens": 0,
            "wait_time": 0.0, "encode_time": 0.0
        }
        t_main_start = time.time()

        def send_chunk(idx):
            s, e = idx * samples_per_chunk, min((idx + 1) * samples_per_chunk, total_len)
            data = audio[s:e]
            if len(data) < samples_per_chunk: 
                data = np.pad(data, (0, samples_per_chunk - len(data)))
            self.to_enc_q.put(StreamingMessage(MsgType.CMD_ENCODE, data=data, is_last=(idx == num_chunks - 1)))

        if num_chunks > 0: send_chunk(0)

        for i in range(num_chunks):
            # è·å–ç‰¹å¾ (ç”Ÿäº§è€…-æ¶ˆè´¹è€…)
            t_w_start = time.time()
            msg = self.from_enc_q.get()
            stats["wait_time"] += (time.time() - t_w_start)
            stats["encode_time"] += msg.encode_time
            
            current_embd = msg.data
            was_last = msg.is_last
            
            # æå‰è§¦å‘ä¸‹ä¸€å—
            if not was_last: send_chunk(i + 1)
            
            # æ„å»ºè®°å¿†
            prefix_text = "".join([seg['text'] for seg in history_segments])
            combined_audio = np.concatenate([seg['embd'] for seg in history_segments] + [current_embd], axis=0)
            
            full_embd = self._build_prompt_embd(combined_audio, prefix_text, context, language)
            
            # LLM æ¨ç† (å¸¦ç†”æ–­é‡å¯æœºåˆ¶)
            temp = temperature
            for retry in range(6):
                res = self._run_llm_buffered(full_embd, prefix_text, rollback_num, is_last_chunk=was_last, temperature=temp)
                if not res.is_aborted: break
                temp += 0.3
                if self.verbose: print(f"\n[ASR] ç†”æ–­é‡å¯ (Temp={temp:.1f})")
            
            # æ›´æ–°æ–‡æœ¬ä¸ç»Ÿè®¡
            new_text_part = res.text[len(prefix_text):]
            history_segments.append({'embd': current_embd, 'text': new_text_part})
            total_full_text += new_text_part
            
            stats["prefill_tokens"] += res.n_prefill; stats["prefill_time"] += res.t_prefill
            stats["decode_tokens"] += res.n_generate; stats["decode_time"] += res.t_generate

        t_total = time.time() - t_main_start
        audio_duration = total_len / sr

        if self.verbose:
            rtf = t_total / audio_duration if audio_duration > 0 else 0
            pre_speed = stats["prefill_tokens"] / (stats["prefill_time"]/1000) if stats["prefill_time"] > 0 else 0
            gen_speed = stats["decode_tokens"] / (stats["decode_time"]/1000) if stats["decode_time"] > 0 else 0
            
            print(f"\n\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"  ğŸ”¹ RTF (å®æ—¶ç‡) : {rtf:.3f} (è¶Šå°è¶Šå¿«)")
            print(f"  ğŸ”¹ éŸ³é¢‘æ—¶é•¿    : {audio_duration:.2f} ç§’")
            print(f"  ğŸ”¹ æ€»å¤„ç†è€—æ—¶  : {t_total:.2f} ç§’")
            print(f"  ğŸ”¹ ç¼–ç ç­‰å¾…    : {stats['wait_time']:.2f} ç§’ (ç­‰å¾…éŸ³é¢‘ç‰¹å¾æå–)")
            print(f"  ğŸ”¹ LLM é¢„å¡«å……  : {stats['prefill_time']/1000:.3f} ç§’ ({stats['prefill_tokens']} tokens, {pre_speed:.1f} tokens/s)")
            print(f"  ğŸ”¹ LLM ç”Ÿæˆ    : {stats['decode_time']/1000:.3f} ç§’ ({stats['decode_tokens']} tokens, {gen_speed:.1f} tokens/s)")
            
        return total_full_text
