# Qwen3-ASR LLM (Thinker) å¯¼å‡ºç»éªŒæ€»ç»“

## 1. æ ¸å¿ƒå‘ç°ï¼šQwen3-ASR çš„çœŸå®èº«ä»½

åœ¨å°è¯•å°† Qwen3-ASR çš„ LLM éƒ¨åˆ†å¯¼å‡ºä¸º GGUF æ—¶ï¼Œæˆ‘ä»¬å‘ç°å®ƒå¹¶ä¸æ˜¯æ ‡å‡†çš„ `Qwen2` æ¨¡å‹ï¼Œè€Œæ˜¯å…·å¤‡ä»¥ä¸‹ç‰¹å¾ï¼š

*   **Q-Norm / K-Norm**: Attention å±‚åŒ…å« `q_norm` å’Œ `k_norm`ã€‚
*   **MRoPE (Multimodal RoPE)**: ä½¿ç”¨äº† 3D ä½ç½®ç¼–ç é€»è¾‘ï¼ˆè™½ç„¶éŸ³é¢‘æ˜¯ 1D çš„ï¼Œä½†ä»£ç æ²¿ç”¨äº† Omni/VL çš„ç»“æ„ï¼‰ã€‚

è¿™äº›ç‰¹å¾è¡¨æ˜å®ƒå±äº **Qwen3-VL** æ¶æ„ç³»åˆ—ã€‚è¿™æ˜¯ç›¸å¯¹è¾ƒæ–°çš„æ¶æ„ï¼Œè™½ç„¶åœ¨ç®—å­å±‚é¢ï¼ˆå¦‚ IMRoPE, RMSNormï¼‰ä¸ Qwen2-VL æœ‰ç»§æ‰¿å…³ç³»ï¼Œä½†å®ƒæ˜¯ç‹¬ç«‹çš„æ¨¡å‹ç±»å‹ã€‚


## 2. å¯¼å‡ºç­–ç•¥ (Export Strategy)

### 2.1 æ¶æ„é…ç½®
åœ¨å¯¼å‡º `config.json` æ—¶ï¼Œæ˜ç¡®æŒ‡å®šæ–°æ¶æ„ï¼š
*   `architectures`: è®¾ç½®ä¸º `["Qwen3VLForConditionalGeneration"]`ã€‚
*   `model_type`: è®¾ç½®ä¸º `qwen3_vl`ã€‚

### 2.2 æƒé‡æ˜ å°„
åŸå§‹æ¨¡å‹å°† LLM åŒ…è£…åœ¨ `thinker` ä¸‹ï¼Œå¯¼å‡ºæ—¶éœ€è¦â€œå‰¥å£³â€ï¼š
*   `thinker.model.*` -> `model.*`
*   `thinker.lm_head.*` -> `lm_head.*`

### 2.3 å‘ç‚¹ï¼šSafeTensors å…±äº«å†…å­˜æŠ¥é”™
**é—®é¢˜**ï¼š`Qwen3` çš„ `lm_head` å’Œ `embed_tokens` æƒé‡é€šå¸¸æ˜¯ç»‘å®šçš„ï¼ˆå…±äº«å†…å­˜ï¼‰ã€‚`safetensors` ä¸æ”¯æŒä¿å­˜å…±äº«å†…å­˜çš„å¼ é‡ï¼Œä¼šæŠ¥ `RuntimeError: Some tensors share memory`ã€‚
**è§£å†³**ï¼šåœ¨å¯¼å‡ºæ—¶ï¼Œå¯¹ `lm_head.weight` è¿›è¡Œ `.clone()`ï¼Œå¼ºåˆ¶åˆ†é…ç‹¬ç«‹å†…å­˜ã€‚
```python
# 21-Export-ASR-LLM.py
if key.startswith("thinker.lm_head."):
    new_key = key.replace("thinker.lm_head.", "lm_head.")
    # Clone to separate memory from embed_tokens if they are tied
    new_state_dict[new_key] = as_state_dict[key].clone()
```

## 3. éªŒè¯ç­–ç•¥ (Verification Strategy)

### 3.1 å‘ç‚¹ï¼šTransformers æ³¨å†Œå†²çª
**é—®é¢˜**ï¼šåœ¨éªŒè¯è„šæœ¬ä¸­ï¼Œå¦‚æœä½ å°è¯•ç”¨ `AutoConfig.register("qwen3_vl", ...)` æ¥æ³¨å†Œä¸€ä¸ªè‡ªå®šä¹‰ç±»ï¼Œè€Œ `transformers` å†…éƒ¨å·²ç»ä¿ç•™äº†è¿™ä¸ªåå­—ï¼ˆæˆ–æ£€æµ‹åˆ°å†²çªï¼‰ï¼Œä¼šæŠ¥é”™ `ValueError: 'qwen3_vl' is already used`ã€‚

**å°è¯•å¤±è´¥çš„æ–¹æ¡ˆ**ï¼š
```python
# âŒ è¿™ç§å†™æ³•ä¼šå’Œ transformers å†…ç½® registry æ‰“æ¶
AutoConfig.register("qwen3_vl", MyConfig)
AutoModel.from_pretrained(path)
```

### 3.2 è§£å†³æ–¹æ¡ˆï¼šå®šä¹‰åŸç”Ÿ Standalone ç±»å¹¶ä½¿ç”¨ `from_pretrained`
**æˆåŠŸæ–¹æ¡ˆ**ï¼šåœ¨é¡¹ç›®ä¸­å®šä¹‰ä¸€ä¸ªä¸“ç”¨çš„ `Qwen3ASRStandaloneLLM` ç±»ï¼Œé€šè¿‡ç»§æ‰¿ `Qwen3ASRThinkerTextPreTrainedModel` (åŸºç±») å¹¶ç»„åˆ `Qwen3ASRThinkerTextModel` (Backbone) ä¸ `lm_head`ï¼Œå¯ä»¥ç›´æ¥åˆ©ç”¨å®˜æ–¹çš„ `from_pretrained` æ–¹æ³•è¿›è¡Œä¸€é”®åŠ è½½ã€‚

```python
# âœ… è¿™ç§å†™æ³•æœ€æ ‡å‡†ï¼Œæƒé‡è‡ªåŠ¨åŒ¹é…
from qwen_asr.core.transformers_backend.modeling_qwen3_asr import (
    Qwen3ASRThinkerTextModel, 
    Qwen3ASRThinkerTextPreTrainedModel
)

class Qwen3ASRStandaloneLLM(Qwen3ASRThinkerTextPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen3ASRThinkerTextModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

# ç›´æ¥åŠ è½½å…¨å¥—æƒé‡ï¼ˆåŒ…å« model.* å’Œ lm_head.*ï¼‰
model = Qwen3ASRStandaloneLLM.from_pretrained(model_path, config=config)
```

## 4. GGUF è½¬æ¢é˜¶æ®µ (GGUF Conversion)

### 4.1 æ ¸å¿ƒæŒ‘æˆ˜ï¼šé…ç½®åŠ è½½â€œå¼ å† ææˆ´â€
åœ¨åˆæ¬¡è½¬æ¢æ—¶ï¼Œæ—¥å¿—æ˜¾ç¤º `embedding length = 4096`ï¼Œè€Œæˆ‘ä»¬å®é™…æ˜¯ **2048**ã€‚
**åŸå› **ï¼šè½¬æ¢å™¨è°ƒç”¨çš„ `AutoConfig.from_pretrained` è¯†åˆ«åˆ° `model_type="qwen3_vl"` åï¼Œç”±äºæœ¬åœ°åº“æˆ–è¿œç¨‹é…ç½®çš„å¹²æ‰°ï¼ŒåŠ è½½äº†ä¸€ä¸ªé»˜è®¤çš„å¤§æ¨¡å‹é…ç½®ï¼Œå¯¼è‡´å‚æ•°å®Œå…¨å¤±é…ï¼Œä¸”ä¸¢å¤±äº† `mrope_section`ã€‚

### 4.2 è§£å†³æ–¹æ¡ˆï¼šçŒ´å­è¡¥ä¸ (Monkey Patching)
åœ¨è½¬æ¢è„šæœ¬ä¸­æ³¨å…¥ä»¥ä¸‹è¡¥ä¸ï¼Œå¼ºåˆ¶è½¬æ¢å™¨è®¤å‡†æœ¬åœ°æ–‡ä»¶ï¼š

```python
# 23-Convert-LLM-GGUF.py ä¸­çš„æ ¸å¿ƒè¡¥ä¸
def patched_load_hparams(dir_model: Path, is_mistral_format: bool):
    print(f"ğŸ’‰ [è¡¥ä¸] å¼ºåˆ¶ä»æœ¬åœ° config.json åŠ è½½å‚æ•°")
    with open(dir_model / "config.json", "r", encoding="utf-8") as f:
        return json.load(f)

# åº”ç”¨è¡¥ä¸
ModelBase.load_hparams = staticmethod(patched_load_hparams)
TextModel.get_vocab_base_pre = lambda self, tok: "qwen2" # å¼ºåˆ¶è¯†åˆ«åˆ†è¯å™¨
```

### 4.3 éªŒè¯ç»“æœ (llama-bench)
è½¬æ¢åçš„æ¨¡å‹åœ¨ `llama.cpp` ä¸­æˆåŠŸåŠ è½½ï¼Œæ¨ç†æ€§èƒ½ï¼ˆVulkan åç«¯ï¼‰ï¼š
- **PP512** (Prompt Processing): ~6927 t/s
- **TG128** (Text Generation): ~82 t/s
- **å‚æ•°è§„æ¨¡**: 2.03 B (F16)

## 5. æ€»ç»“ä¸äº§ç‰©
*   **æµç¨‹**: æƒé‡é‡å‘½å (21) -> åŸç”Ÿä»£ç éªŒè¯ (22) -> è¡¥ä¸è¾…åŠ©è½¬æ¢ (23)
*   **è„šæœ¬**: 
    - [21-Export-ASR-LLM.py](file:///d:/qwen3-asr/21-Export-ASR-LLM.py)
    - [22-Verify-ASR-LLM.py](file:///d:/qwen3-asr/22-Verify-ASR-LLM.py)
    - [23-Convert-LLM-GGUF.py](file:///d:/qwen3-asr/23-Convert-LLM-GGUF.py)
*   **æœ€åä¸€æ­¥**: ç¡®ä¿ GGUF è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°äº† `gguf: mrope sections: [24, 20, 20, 0]`ï¼Œè¿™æ˜¯ ASR æ­£å¸¸å·¥ä½œçš„åŸºçŸ³ã€‚
