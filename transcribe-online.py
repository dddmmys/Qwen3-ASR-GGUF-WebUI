
import os
import sys
import time
import codecs
import re
import numpy as np
import multiprocessing as mp
from pathlib import Path
from collections import deque
from dataclasses import dataclass
from enum import Enum, auto
from typing import Any, Optional
from qwen_asr_gguf.inference import chinese_itn

# ==================== Vulkan é€‰é¡¹ ====================

# os.environ["VK_ICD_FILENAMES"] = "none"       # ç¦æ­¢ Vulkan
# os.environ["GGML_VK_VISIBLE_DEVICES"] = "1"   # ç¦æ­¢ Vulkan ç”¨ç‹¬æ˜¾ï¼ˆå¼ºåˆ¶ç”¨é›†æ˜¾ï¼‰
# os.environ["GGML_VK_DISABLE_F16"] = "1"       # ç¦æ­¢ VulkanFP16 è®¡ç®—ï¼ˆIntelé›†æ˜¾fp16æœ‰æº¢å‡ºé—®é¢˜ï¼‰


# ==========================================
# 1. åè®®å®šä¹‰ (Dataclasses)
# ==========================================
class MsgType(Enum):
    CMD_ENCODE = auto()   # ä¸»è¿›ç¨‹ -> Encoder: ç¼–ç è¯·æ±‚
    CMD_STOP = auto()     # ä¸»è¿›ç¨‹ -> Encoder: åœæ­¢è¯·æ±‚
    MSG_EMBD = auto()     # Encoder -> ä¸»è¿›ç¨‹: è¿”å›ç‰¹å¾
    MSG_READY = auto()    # Encoder -> ä¸»è¿›ç¨‹: å°±ç»ªä¿¡å·
    MSG_DONE = auto()     # Encoder -> ä¸»è¿›ç¨‹: å·²é€€å‡ºä¿¡å·

@dataclass
class StreamingMessage:
    msg_type: MsgType
    data: Any = None      # å­˜æ”¾éŸ³é¢‘ chunk æˆ– embedding ç»“æœ
    is_last: bool = False # æ ‡è®°æ˜¯å¦ä¸ºæœ€åä¸€æ®µéŸ³é¢‘
    encode_time: float = 0.0 # ç¼–ç å™¨å®é™…è€—æ—¶

@dataclass
class LLMDecodeResult:
    """LLM è§£ç ç»“æœï¼ˆå†…æ ¸è¾“å‡ºæ ‡å‡†åŒ–ï¼‰"""
    text: str = ""           # ç”Ÿæˆçš„æ–‡æœ¬
    stable_tokens: list = None  # ç¨³å®š token åˆ—è¡¨
    t_prefill: float = 0.0   # é¢„å¡«å……è€—æ—¶
    t_generate: float = 0.0  # ç”Ÿæˆè€—æ—¶
    n_prefill: int = 0       # é¢„å¡«å…… token æ•°
    n_generate: int = 0      # ç”Ÿæˆ token æ•°
    is_aborted: bool = False # æ˜¯å¦å› ç†”æ–­è€Œä¸­æ­¢
    
    def __post_init__(self):
        if self.stable_tokens is None:
            self.stable_tokens = []

# ==========================================
# 2. ç¼–ç å™¨è¿›ç¨‹ (Encoder Worker & Preprocessor)
# ==========================================
class FastWhisperMel:
    """å®Œå…¨åŸºäº NumPy å’Œ Librosa çš„ Mel æå–å™¨ (æ›¿ä»£ Transformers)"""
    def __init__(self, filter_path):
        self.filters = np.load(filter_path) # (201, 128)
        
    def __call__(self, audio, dtype=np.float32):
        import librosa
        # 1. STFT (Reflect padding, Hann window)
        stft = librosa.stft(audio, n_fft=400, hop_length=160, window='hann', center=True)
        # 2. Power Spectrum
        magnitudes = np.abs(stft) ** 2
        # 3. Mel Filterbank ( official filters are (201, 128) )
        mel_spec = np.dot(self.filters.T, magnitudes)
        # 4. Log Mel
        log_spec = np.log10(np.maximum(mel_spec, 1e-10))
        # 5. Normalization
        log_spec = np.maximum(log_spec, log_spec.max() - 8.0)
        log_spec = (log_spec + 4.0) / 4.0
        # 6. å¸§å¯¹é½ï¼šä¸¢å¼ƒ stft(center=True) äº§ç”Ÿçš„å¤šä½™å¸§
        n_frames = audio.shape[-1] // 160
        log_spec = log_spec[:, :n_frames]
        return log_spec.astype(dtype)

def _get_feat_lengths(t):
    """è®¡ç®—ä¸‹é‡‡æ ·åçš„é•¿åº¦ (ç”¨äºç”Ÿæˆæ³¨æ„æ©ç )"""
    t_leave = t % 100
    feat_len = (t_leave - 1) // 2 + 1
    out_len = ((feat_len - 1) // 2 + 1 - 1) // 2 + 1 + (t // 100) * 13
    return int(out_len)

def encoder_worker_proc(to_enc_q, from_enc_q, encoder_path, mel_filters_path):
    """å•æ¨¡å‹ç¼–ç è¿›ç¨‹ï¼šæ”¯æŒåˆå¹¶ç‰ˆ Encoderï¼Œé»˜è®¤å¼€å¯ DirectML"""
    import onnxruntime as ort
    
    # 1. åˆå§‹åŒ–è®¾ç½®
    sess_opts = ort.SessionOptions()
    sess_opts.log_severity_level = 3
    sess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

    # ä¼˜å…ˆå°è¯•ä½¿ç”¨ DirectML
    providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
    try:
        encoder_sess = ort.InferenceSession(encoder_path, sess_options=sess_opts, providers=providers)
        used_provider = encoder_sess.get_providers()[0]
        print(f"[ç¼–ç è¿›ç¨‹] æ¨¡å‹å·²åŠ è½½ï¼Œå½“å‰ EP: {used_provider}")
    except Exception as e:
        print(f"[ç¼–ç è¿›ç¨‹] åŠ è½½åˆå¹¶ç‰ˆ ONNX å¤±è´¥: {e}")
        return
    
    mel_extractor = FastWhisperMel(mel_filters_path)
    
    # æ£€æµ‹æ¨¡å‹è¾“å…¥ç²¾åº¦
    try:
        fe_input_type = encoder_sess.get_inputs()[0].type
        input_dtype = np.float16 if 'float16' in fe_input_type else np.float32
    except:
        input_dtype = np.float32
        print(f"ç¼–ç è¿›ç¨‹é»˜è®¤è¾“å…¥ç²¾åº¦: float32")

    # GPU Warmup: è·‘ä¸€æ®µéŸ³é¢‘ä»¥è§¦å‘ Shader ç¼–è¯‘å’Œæ˜¾å­˜åˆ†é…


    # GPU Warmup: è·‘ä¸€æ®µéŸ³é¢‘ä»¥è§¦å‘ Shader ç¼–è¯‘å’Œæ˜¾å­˜åˆ†é…
    warmup_seconds = 5
    dummy_wav = np.random.randn(int(16000 * warmup_seconds)).astype(np.float32)
    try:
        # æ¨¡æ‹Ÿå®Œæ•´çš„æ¨ç†æµç¨‹
        dummy_mel = mel_extractor(dummy_wav, dtype=input_dtype)
        dummy_mel_input = dummy_mel[np.newaxis, ...]
        t_out = _get_feat_lengths(dummy_mel.shape[1])
        dummy_mask = np.zeros((1, 1, t_out, t_out), dtype=np.float32)
        
        _ = encoder_sess.run(None, {
            "input_features": dummy_mel_input,
            "attention_mask": dummy_mask
        })[0]
        print(f"[ç¼–ç è¿›ç¨‹] DirectML é¢„çƒ­å®Œæˆ (5ç§’éŸ³é¢‘)")
    except Exception as e:
        print(f"[ç¼–ç è¿›ç¨‹] é¢„çƒ­å¤±è´¥ (å·²å¿½ç•¥): {e}")

    # å‘é€å°±ç»ªä¿¡å·
    from_enc_q.put(StreamingMessage(MsgType.MSG_READY))
    
    while True:
        msg: StreamingMessage = to_enc_q.get()
        
        if msg.msg_type == MsgType.CMD_STOP:
            from_enc_q.put(StreamingMessage(MsgType.MSG_DONE))
            break
            
        if msg.msg_type == MsgType.CMD_ENCODE:
            audio_chunk = msg.data
            t0 = time.time()
            
            # A. æå– Mel (B, 128, T)
            mel = mel_extractor(audio_chunk, dtype=input_dtype) 
            mel_input = mel[np.newaxis, ...] # (1, 128, T)
            
            # B. è®¡ç®—æ©ç  (B, 1, T_out, T_out)
            t_mel = mel.shape[1]
            t_out = _get_feat_lengths(t_mel)
            mask_input = np.zeros((1, 1, t_out, t_out), dtype=np.float32)
            
            # C. æ¨ç† (åˆå¹¶ç‰ˆ Encoder)
            audio_embd = encoder_sess.run(None, {
                "input_features": mel_input,
                "attention_mask": mask_input
            })[0]
            
            if audio_embd.ndim == 3: audio_embd = audio_embd[0] # (T_out, 1024)
            
            t_encode = time.time() - t0
            from_enc_q.put(StreamingMessage(MsgType.MSG_EMBD, data=audio_embd, is_last=msg.is_last, encode_time=t_encode))

# ==========================================
# 3. è¾…åŠ©å‡½æ•°: Pydub éŸ³é¢‘åŠ è½½
# ==========================================
def load_audio(audio_path, sample_rate=16000, start_second=None, duration=None):
    """åŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶è½¬æ¢ä¸º 16kHz PCMï¼Œæ”¯æŒæŒ‰éœ€åŠ è½½æŒ‡å®šç‰‡æ®µ"""
    from pydub import AudioSegment
    
    # ä½¿ç”¨ pydub çš„ start_second å’Œ duration å‚æ•°æ¥å‡å°‘è§£ç é‡ï¼ˆå¦‚æœç¯å¢ƒæ”¯æŒï¼‰
    # å¦‚æœç¯å¢ƒä¸­çš„ pydub ä¸æ”¯æŒè¿™äº›å‚æ•°ï¼Œå®ƒä»¬ä¼šè¢«å¿½ç•¥æˆ–æŠ¥é”™ï¼Œè¿™é‡Œé€šè¿‡ kwargs ä¼ é€’æ›´ç¨³å¥
    load_kwargs = {
        "frame_rate": sample_rate, 
        "channels": 1
    }
    if start_second is not None: load_kwargs['start_second'] = start_second
    if duration: load_kwargs['duration'] = duration

    audio_segment = AudioSegment.from_file(audio_path, **load_kwargs)

    bit_depth = audio_segment.sample_width * 8
    max_val = float(1 << (bit_depth - 1))
    
    audio = np.array(
        audio_segment
        .set_channels(1)
        .set_frame_rate(sample_rate)
        .get_array_of_samples(),
    ) / max_val

    return audio

# ==========================================
# 4. æ ¸å¿ƒæµå¼å™¨ (Engine)
# ==========================================
class ChunkSegment:
    def __init__(self, audio_embd):
        self.audio_embd = audio_embd
        self.committed_text = "" # è¯¥ç‰‡æ®µé”å®šçš„ç¨³å®šæ–‡æœ¬

class QwenASREngine:
    def __init__(
        self,
        encoder_path: str,
        llm_gguf_path: str,
        mel_filters_path: str,
        verbose: bool = True
    ):
        t_start = time.time()
        print(f"--- åˆå§‹åŒ– QwenASR å¼•æ“ ---")
        self.verbose = verbose
        
        # å»¶è¿Ÿå¯¼å…¥ LLM ç»„ä»¶
        from qwen_asr_gguf.inference import llama
        self.llama_mod = llama # keep reference
        
        # åŠ è½½ LLM
        if verbose: print(f"æ­£åœ¨åŠ è½½ LLM: {llm_gguf_path}")
        self.model = llama.LlamaModel(llm_gguf_path)
        self.embedding_table = llama.get_token_embeddings_gguf(llm_gguf_path)
        self.ctx = llama.LlamaContext(self.model, n_ctx=4096, n_batch=4096, embeddings=False)
        
        # å»ºç«‹æ¶ˆæ¯é˜Ÿåˆ—
        self.to_enc_q = mp.Queue()
        self.from_enc_q = mp.Queue()
        
        # å¯åŠ¨ç¼–ç å™¨è¿›ç¨‹ (åˆå¹¶ç‰ˆ Encoder)
        if verbose: print("æ­£åœ¨å¯åŠ¨éŸ³é¢‘ç¼–ç è¿›ç¨‹ (Full Encoder)...")
        self.enc_proc = mp.Process(
            target=encoder_worker_proc, 
            args=(self.to_enc_q, self.from_enc_q, encoder_path, mel_filters_path), 
            daemon=True
        )
        self.enc_proc.start()
        
        # ç­‰å¾…å°±ç»ª
        msg = self.from_enc_q.get()
        if msg.msg_type == MsgType.MSG_READY:
            if verbose: print("éŸ³é¢‘ç¼–ç è¿›ç¨‹å°±ç»ªã€‚")
        
        self.load_time = time.time() - t_start
        
        # åŸºç¡€ Token IDç¼“å­˜
        self.ID_IM_START = self.model.token_to_id("<|im_start|>")
        self.ID_IM_END = self.model.token_to_id("<|im_end|>")
        self.ID_AUDIO_START = self.model.token_to_id("<|audio_start|>")
        self.ID_AUDIO_END = self.model.token_to_id("<|audio_end|>")
        self.ID_ASR_TEXT = self.model.token_to_id("<asr_text>")

    def build_prompt_embeddings(self, audio_embd: np.ndarray, prefix_text: str, context: Optional[str], language: Optional[str]) -> np.ndarray:
        """æ„å»ºå®Œæ•´çš„ Prompt Embedding çŸ©é˜µï¼ˆæ”¯æŒå³æ’å³ç”¨ï¼‰"""
        system_text = "You are a helpful assistant. "
        user_prompt_text = f"{context}\n\n" if context else ""

        def tk(t): return self.model.tokenize(t)

        # 1. å‰ç¼€ Token
        prefix_tokens = [self.ID_IM_START] + tk(f"system\n{system_text}") + [self.ID_IM_END] + \
                        [self.ID_IM_START] + tk(f"user\n{user_prompt_text}") + [self.ID_AUDIO_START]
        
        # 2. åç¼€ Tokenï¼ˆåŒ…å«è¯­è¨€å¼•å¯¼å’Œå·²è½¬å½•çš„éƒ¨åˆ†æ–‡æœ¬ï¼‰
        assistant_prompt = "assistant\n"
        if language:
            assistant_prompt += f"language {language}"

        suffix_tokens = [self.ID_AUDIO_END] + tk("æ•°å­—ç”¨0123456789ï¼Œè¯­éŸ³è½¬å½•ï¼š") + [self.ID_IM_END] + \
                        [self.ID_IM_START] + tk(assistant_prompt) + [self.ID_ASR_TEXT] + \
                        tk(prefix_text)

        n_prefix = len(prefix_tokens)
        n_audio = audio_embd.shape[0]
        n_suffix = len(suffix_tokens)
        total_len = n_prefix + n_audio + n_suffix
        
        # 3. æ‹¼æ¥ Embedding
        full_embd = np.zeros((total_len, self.model.n_embd), dtype=np.float32)
        full_embd[:n_prefix] = self.embedding_table[prefix_tokens]
        full_embd[n_prefix : n_prefix + n_audio] = audio_embd
        full_embd[n_prefix + n_audio : n_prefix + n_audio + n_suffix] = self.embedding_table[suffix_tokens]
        
        return full_embd

    def shutdown(self):
        self.to_enc_q.put(StreamingMessage(MsgType.CMD_STOP))
        msg = self.from_enc_q.get()
        if msg.msg_type == MsgType.MSG_DONE:
            if self.verbose: print("\nç¼–ç è¿›ç¨‹å·²å®‰å…¨ç»ˆæ­¢ã€‚")
        self.enc_proc.join()

    def _run_llm_buffered(
        self, 
        full_embd: np.ndarray,
        prefix_text: str, 
        rollback_num: int,
        is_last_chunk: bool = False, 
        temperature: float = 0.4
    ) -> LLMDecodeResult:
        """å†…éƒ¨æ–¹æ³•ï¼šæ‰§è¡Œå•æ¬¡ LLM ç”Ÿæˆå¾ªç¯ï¼ˆä»…è´Ÿè´£æ¨ç†ï¼‰"""
        result = LLMDecodeResult()
        
        total_len = full_embd.shape[0]
        pos_base = np.arange(0, total_len, dtype=np.int32)
        pos_arr = np.concatenate([pos_base, pos_base, pos_base, np.zeros(total_len, dtype=np.int32)])
        batch = self.llama_mod.LlamaBatch(max(total_len * 4, 8192), self.model.n_embd, 1)
        batch.set_embd(full_embd, pos=pos_arr)
        
        # 1. Prefill
        self.ctx.clear_kv_cache()
        t_pre_start = time.time()
        self.ctx.decode(batch)
        prefill_time = time.time() - t_pre_start
        
        # 2. Generation Loopï¼ˆä½¿ç”¨æ–°é‡‡æ ·å™¨å’Œéšæœºç§å­ï¼‰
        t_gen_start = time.time()
        n_gen_tokens = 0
        display_queue = deque()
        stable_tokens = []
        stable_text_acc = ""
        cur_pos = total_len
        gen_batch = self.llama_mod.LlamaBatch(4, 0, 1)
        text_decoder = codecs.getincrementaldecoder('utf-8')(errors='replace')
        
        # æ¯æ¬¡è§£ç ä½¿ç”¨æ–°çš„éšæœºç§å­
        seed = int(np.random.randint(0, 2**31 - 1))
        sampler = self.llama_mod.LlamaSampler(temperature=temperature, seed=seed)
        last_sampled_token = sampler.sample(self.ctx.ptr)
        for _ in range(150): # Max new tokens per chunk
            if last_sampled_token in [self.model.eos_token, self.ID_IM_END]:
                break
            
            gen_batch.set_token(last_sampled_token, pos=np.array([cur_pos, cur_pos, cur_pos, 0], dtype=np.int32))
            self.ctx.decode(gen_batch)
            
            display_queue.append(last_sampled_token)
            if len(display_queue) > rollback_num:
                ready_token = display_queue.popleft()
                stable_tokens.append(ready_token)
                piece = text_decoder.decode(self.model.token_to_bytes(ready_token))
                end = '\n' if re.search('[ï¼Œã€‚ï¼Ÿï¼]', piece) else ''
                if piece:
                    print(piece, end=end, flush=True)
                    stable_text_acc += piece
            
            # ç†”æ–­æ£€æŸ¥ï¼šæ£€æµ‹é‡å¤å¾ªç¯
            if len(stable_tokens) > 15:
                if len(set(stable_tokens[-15:])) <= 3:
                    result.is_aborted = True
                    break
            
            cur_pos += 1
            last_sampled_token = sampler.sample(self.ctx.ptr)
            n_gen_tokens += 1
            
        gen_time = time.time() - t_gen_start
        del sampler  # é‡Šæ”¾é‡‡æ ·å™¨èµ„æº
            
        if is_last_chunk and not result.is_aborted:
            while display_queue:
                t = display_queue.popleft()
                stable_tokens.append(t)
                piece = text_decoder.decode(self.model.token_to_bytes(t))
                if piece:
                    print(piece, end="", flush=True)
                    stable_text_acc += piece
            final_p = text_decoder.decode(b"", final=True)
            if final_p:
                end = '\n' if re.search('[ï¼Œã€‚ï¼Ÿï¼]', piece) else ''
                print(final_p, end=end, flush=True)
                stable_text_acc += final_p
        
        # å¡«å……ç»“æœï¼ˆå†…æ ¸è¾“å‡ºæ ‡å‡†åŒ–ï¼‰
        result.text = prefix_text + stable_text_acc
        result.stable_tokens = stable_tokens
        result.t_prefill = prefill_time
        result.t_generate = gen_time
        result.n_prefill = total_len
        result.n_generate = n_gen_tokens
        return result

    def transcribe(
        self, 
        audio_file: str, 
        language: str = None, 
        context: str = None, 
        chunk_size: float = 40.0,
        start_second: float = 0.0,
        duration: float = None,
        temperature: float = 0.4,
        memory_num: int = 2,    # è®°å¿†ä¸­ä¿ç•™çš„éŸ³é¢‘ç‰‡æ®µæ•°é‡
        rollback_num: int = 5   # å›æ»š/æ’¤é”€çš„ Token æ•°é‡
    ) -> str:
        
        if self.verbose:
            print(f"\næ­£åœ¨å¤„ç†: {audio_file}")
            print(f"å‚æ•°é…ç½®: åˆ‡ç‰‡={chunk_size}s, è®°å¿†æ•°={memory_num}, æ¸©åº¦={temperature}, è¯­è¨€={language}, èµ·å§‹={start_second}s, æ—¶é•¿={duration}s")


        # åŠ è½½éŸ³é¢‘ (ä½¿ç”¨ Pydub)
        full_audio = load_audio(audio_file, sample_rate=16000, start_second=start_second, duration=duration)
        sr = 16000

        SAMPLES_PER_CHUNK = int(chunk_size * sr)
        total_len = len(full_audio)
        num_chunks = int(np.ceil(total_len / SAMPLES_PER_CHUNK))
        
        # çŠ¶æ€é‡ç½®
        segment_queue = deque(maxlen=memory_num)
        total_full_text = ""
        
        # ç»Ÿè®¡
        stats = {
            "prefill_time": 0.0, "decode_time": 0.0,
            "prefill_tokens": 0, "decode_tokens": 0,
            "wait_time": 0.0, "encode_time": 0.0
        }
        
        t_main_start = time.time()
        
        # --- å†…éƒ¨ Chunk è·å–å‡½æ•° ---
        def get_chunk(idx):
            s = idx * SAMPLES_PER_CHUNK
            e = min((idx+1) * SAMPLES_PER_CHUNK, total_len)
            chunk = full_audio[s:e]
            if len(chunk) < SAMPLES_PER_CHUNK:
                chunk = np.pad(chunk, (0, SAMPLES_PER_CHUNK - len(chunk)))
            return chunk, (idx == num_chunks - 1)

        print("--- å¼€å§‹æµå¼è½¬å½• ---")
        
        # 1. å‘é€ç¬¬ä¸€ä¸ªå—
        if num_chunks > 0:
            chunk, is_last = get_chunk(0)
            self.to_enc_q.put(StreamingMessage(MsgType.CMD_ENCODE, data=chunk, is_last=is_last))
        
        for i in range(num_chunks):
            # 2. ç­‰å¾…å½“å‰å—çš„ Embedding
            t_w_start = time.time()
            msg: StreamingMessage = self.from_enc_q.get()
            stats["wait_time"] += (time.time() - t_w_start)
            stats["encode_time"] += msg.encode_time
            
            current_embd = msg.data
            was_last = msg.is_last
            
            # 3. æ¡æ‰‹è§¦å‘ï¼šç«‹åˆ»å‘é€ä¸‹ä¸€å—çš„ç¼–ç æŒ‡ä»¤ï¼ˆå¦‚æœæœ‰ï¼‰
            if not was_last:
                next_chunk, next_is_last = get_chunk(i + 1)
                self.to_enc_q.put(StreamingMessage(MsgType.CMD_ENCODE, data=next_chunk, is_last=next_is_last))
            
            # 4. LLM è§£ç 
            new_seg = ChunkSegment(current_embd)
            segment_queue.append(new_seg)
            
            # åªä½¿ç”¨è®°å¿†çª—å£å†…çš„ç‰‡æ®µæ–‡æœ¬ä½œä¸º prefixï¼ˆä¸åŒ…æ‹¬å½“å‰æ­£åœ¨è§£ç çš„ç‰‡æ®µï¼‰
            prefix_str = "".join([s.committed_text for s in list(segment_queue)[:-1]])
            total_audio_input = np.concatenate([s.audio_embd for s in segment_queue], axis=0)
            
            # 1. å‡†å¤‡ Embedding (èŒè´£åˆ†ç¦»)
            full_embd = self.build_prompt_embeddings(total_audio_input, prefix_str, context, language)
            
            # 2. LLM è§£ç ï¼ˆå¸¦åŠ æ¸©é‡è¯•æœºåˆ¶ï¼‰
            current_temp = temperature
            for retry in range(6):  # æœ€å¤šé‡è¯• 5 æ¬¡
                llm_result = self._run_llm_buffered(
                    full_embd, prefix_str, rollback_num, 
                    is_last_chunk=was_last, temperature=current_temp
                )
                if not llm_result.is_aborted:
                    break
                # ç†”æ–­è§¦å‘ï¼šåŠ æ¸©é‡è¯•
                current_temp += 0.3
                print(f"\n[!] ç†”æ–­è§¦å‘ï¼Œå‡æ¸©é‡è¯• (Temp -> {current_temp:.1f})")
            
            # æ›´æ–° Segment äº§ç”Ÿçš„æ–‡æœ¬ (ä»…ç´¯åŠ å¢é‡éƒ¨åˆ†ï¼Œé¿å…é‡å¤)
            new_text_part = llm_result.text[len(prefix_str):]
            new_seg.committed_text = new_text_part
            total_full_text += new_text_part
            
            stats["prefill_time"] += llm_result.t_prefill
            stats["decode_time"] += llm_result.t_generate
            stats["prefill_tokens"] += llm_result.n_prefill
            stats["decode_tokens"] += llm_result.n_generate

        t_total = time.time() - t_main_start
        audio_duration = total_len / 16000
        
        print('\n\n')
        print('='*10 + 'ITNå¤„ç†ç»“æœ' + '='*10)
        total_full_text = chinese_itn.chinese_to_num(total_full_text)
        print(total_full_text)
        print('='*30)
        
        if self.verbose:
            rtf = t_total / audio_duration if audio_duration > 0 else 0
            prefill_speed = stats["prefill_tokens"] / stats["prefill_time"] if stats["prefill_time"] > 0 else 0
            decode_speed = stats["decode_tokens"] / stats["decode_time"] if stats["decode_time"] > 0 else 0
            
            print(f"\n\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"  ğŸ”¹ RTF (å®æ—¶ç‡) : {rtf:.3f} (è¶Šå°è¶Šå¿«)")
            print(f"  ğŸ”¹ éŸ³é¢‘æ—¶é•¿    : {audio_duration:.2f} ç§’")
            print(f"  ğŸ”¹ æ€»å¤„ç†è€—æ—¶  : {t_total:.2f} ç§’")
            print(f"  ğŸ”¹ ç¼–ç ç­‰å¾…    : {stats['wait_time']:.2f} ç§’ (ç­‰å¾…éŸ³é¢‘ç‰¹å¾æå–)")
            print(f"  ğŸ”¹ LLM é¢„å¡«å……  : {stats['prefill_time']:.3f} ç§’ ({stats['prefill_tokens']} tokens, {prefill_speed:.1f} tokens/s)")
            print(f"  ğŸ”¹ LLM ç”Ÿæˆ    : {stats['decode_time']:.3f} ç§’ ({stats['decode_tokens']} tokens, {decode_speed:.1f} tokens/s)")
            
        return total_full_text

# ==========================================
# 5. ä¸»ç¨‹åº (Example Usage)
# ==========================================
if __name__ == "__main__":
    # Windows ç¯å¢ƒå¤šè¿›ç¨‹å¯åŠ¨ä¼˜åŒ–
    import warnings
    warnings.filterwarnings("ignore")
    
    # å®šä¹‰è·¯å¾„
    PROJECT_ROOT = Path(__file__).parent.absolute()
    encoder_onnx = os.path.join(PROJECT_ROOT, "model", "qwen3_asr_encoder.onnx")
    gguf = os.path.join(PROJECT_ROOT, "model", "qwen3_asr_llm.q8_0.gguf")
    mel_filters = os.path.join(PROJECT_ROOT, "model", "mel_filters.npy")

    # 1. åˆå§‹åŒ–å¼•æ“
    print("æ­£åœ¨åˆå§‹åŒ–å¼•æ“ (DirectML + GGUF)...")
    engine = QwenASREngine(
        encoder_path=encoder_onnx,
        llm_gguf_path=gguf,
        mel_filters_path=mel_filters,
        verbose=True
    )

    # 2. æ‰§è¡Œè½¬å½• (å¯è°ƒç”¨å¤šæ¬¡)
    audio_path = "ç¡å‰æ¶ˆæ¯.m4a"
    
    # ç¤ºä¾‹ï¼šä»…è½¬å½•å‰ 60 ç§’ï¼Œåˆ†å— 40 ç§’
    result_text = engine.transcribe(
        audio_file=audio_path,
        context="",
        language="Chinese", # å¼ºåˆ¶æŒ‡å®šè¯­è¨€ (å¦‚ 'Chinese', 'English', None)
        start_second=0.0,   # ä»ä½•å¤„å¼€å§‹è¯»éŸ³é¢‘
        duration=120,       # è¯»å–å¤šé•¿éŸ³é¢‘ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨è¯»å–
        temperature=0.4,    # LLM Decode æ¸©åº¦
        chunk_size=40.0,    # æ¯ä¸€ç‰‡æ®µçš„æ—¶é•¿
        memory_num=2,       # è®°å¿†å¤šå°‘ç‰‡æ®µ
        rollback_num=5      # è¿æ¥å¤„å›æ»šå‡ ä¸ª TOKEN
    )
    
    
    # 3. èµ„æºæ¸…ç†
    engine.shutdown()
